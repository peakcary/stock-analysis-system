"""
æ•°æ®å¯¼å…¥ç›¸å…³APIç«¯ç‚¹
"""

from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, Query, Form
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.services.data_import import DataImportService
from app.services.ranking_calculator import RankingCalculatorService
from app.models import DataImportRecord
from datetime import date, datetime
from typing import Optional, List

router = APIRouter()


@router.post("/import-csv")
async def import_csv_data(
    file: UploadFile = File(...),
    allow_overwrite: bool = False,
    db: Session = Depends(get_db)
):
    """å¯¼å…¥CSVæ ¼å¼çš„è‚¡ç¥¨æ•°æ®ï¼ˆé‡æ„ç‰ˆæœ¬ - å¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
    
    # éªŒè¯æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
    
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="æ–‡ä»¶å¿…é¡»æ˜¯CSVæ ¼å¼")
    
    # éªŒè¯æ–‡ä»¶å¤§å° (é™åˆ¶100MB)
    if file.size and file.size > 100 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡100MB")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # éªŒè¯æ–‡ä»¶å†…å®¹ä¸ä¸ºç©º
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©º")
        
        # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆå®é™…è¯»å–åï¼‰
        if len(content) > 100 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹è¶…è¿‡100MBé™åˆ¶")
        
        print(f"ğŸ“„ å¼€å§‹å¤„ç†CSVæ–‡ä»¶: {file.filename}, å¤§å°: {len(content)} bytes")
        
        # ä½¿ç”¨æ•°æ®å¯¼å…¥æœåŠ¡å¤„ç†æ–‡ä»¶
        import_service = DataImportService(db)
        result = await import_service.import_csv_data(content, file.filename, allow_overwrite)
        
        print(f"âœ… CSVå¤„ç†å®Œæˆ: å¯¼å…¥{result.get('imported_records', 0)}æ¡, è·³è¿‡{result.get('skipped_records', 0)}æ¡")
        
        if result.get("already_exists"):
            return {
                "message": result["message"],
                "filename": file.filename,
                "imported_records": result["imported_records"],
                "skipped_records": result["skipped_records"],
                "import_date": result["import_date"],
                "already_exists": True
            }
        
        return {
            "message": "CSVæ•°æ®å¯¼å…¥æˆåŠŸ" + ("ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰" if result.get("overwrite") else ""),
            "filename": file.filename,
            "imported_records": result["imported_records"],
            "skipped_records": result["skipped_records"],
            "import_date": result["import_date"],
            "overwrite": result.get("overwrite", False),
            "errors": result.get("errors", [])
        }
        
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        print(f"âŒ CSVå¯¼å…¥å¼‚å¸¸: {str(e)}")
        error_detail = str(e)
        if "CSVè§£æå¤±è´¥" in error_detail:
            raise HTTPException(status_code=400, detail=error_detail)
        else:
            raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {error_detail}")


@router.post("/import-txt")
async def import_txt_data(
    file: UploadFile = File(...),
    allow_overwrite: bool = False,
    db: Session = Depends(get_db)
):
    """å¯¼å…¥TXTæ ¼å¼çš„çƒ­åº¦æ•°æ®ï¼ˆé‡æ„ç‰ˆæœ¬ - å¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
    
    # éªŒè¯æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
    
    if not file.filename.endswith('.txt'):
        raise HTTPException(status_code=400, detail="æ–‡ä»¶å¿…é¡»æ˜¯TXTæ ¼å¼")
    
    # éªŒè¯æ–‡ä»¶å¤§å° (é™åˆ¶50MB)
    if file.size and file.size > 50 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡50MB")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # éªŒè¯æ–‡ä»¶å†…å®¹ä¸ä¸ºç©º
        if not content:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©º")
        
        # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆå®é™…è¯»å–åï¼‰
        if len(content) > 50 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹è¶…è¿‡50MBé™åˆ¶")
        
        print(f"ğŸ“„ å¼€å§‹å¤„ç†TXTæ–‡ä»¶: {file.filename}, å¤§å°: {len(content)} bytes")
        
        # ä½¿ç”¨æ•°æ®å¯¼å…¥æœåŠ¡å¤„ç†æ–‡ä»¶
        import_service = DataImportService(db)
        result = await import_service.import_txt_data(content, file.filename, allow_overwrite)
        
        print(f"âœ… TXTå¤„ç†å®Œæˆ: å¯¼å…¥{result.get('imported_records', 0)}æ¡, è·³è¿‡{result.get('skipped_records', 0)}æ¡")
        
        if result.get("already_exists"):
            return {
                "message": result["message"],
                "filename": file.filename,
                "imported_records": result["imported_records"],
                "skipped_records": result["skipped_records"],
                "import_date": result["import_date"],
                "already_exists": True
            }
        
        return {
            "message": "TXTçƒ­åº¦æ•°æ®å¯¼å…¥æˆåŠŸ" + ("ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰" if result.get("overwrite") else ""),
            "filename": file.filename,
            "imported_records": result["imported_records"],
            "skipped_records": result["skipped_records"],
            "import_date": result["import_date"],
            "overwrite": result.get("overwrite", False),
            "errors": result.get("errors", [])
        }
        
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        print(f"âŒ TXTå¯¼å…¥å¼‚å¸¸: {str(e)}")
        error_detail = str(e)
        if "TXTè§£æå¤±è´¥" in error_detail:
            raise HTTPException(status_code=400, detail=error_detail)
        else:
            raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {error_detail}")


@router.post("/calculate-rankings")
def calculate_rankings(
    trade_date: str,
    db: Session = Depends(get_db)
):
    """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„æ’åå’Œæ¦‚å¿µæ€»å’Œ"""
    try:
        # ä½¿ç”¨æ’åè®¡ç®—æœåŠ¡
        calculator = RankingCalculatorService(db)
        result = calculator.calculate_daily_rankings(trade_date)
        
        return {
            "message": "æ’åè®¡ç®—å®Œæˆ",
            "trade_date": trade_date,
            "processed_concepts": result["processed_concepts"],
            "total_rankings": result["total_rankings"],
            "new_highs_found": result["new_highs_found"]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®¡ç®—å¤±è´¥: {str(e)}")


@router.get("/import-history")
def get_import_history(
    import_date: Optional[str] = Query(None, description="æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„å¯¼å…¥è®°å½• (YYYY-MM-DD)"),
    limit: int = Query(10, description="è¿”å›è®°å½•æ•°é‡é™åˆ¶"),
    db: Session = Depends(get_db)
):
    """è·å–æ•°æ®å¯¼å…¥å†å²è®°å½•"""
    try:
        query = db.query(DataImportRecord)
        
        if import_date:
            try:
                query_date = datetime.strptime(import_date, '%Y-%m-%d').date()
                query = query.filter(DataImportRecord.import_date == query_date)
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD")
        
        records = query.order_by(DataImportRecord.created_at.desc()).limit(limit).all()
        
        result = []
        for record in records:
            result.append({
                "id": record.id,
                "import_date": record.import_date.isoformat(),
                "import_type": record.import_type,
                "file_name": record.file_name,
                "imported_records": record.imported_records,
                "skipped_records": record.skipped_records,
                "import_status": record.import_status,
                "error_message": record.error_message,
                "created_at": record.created_at.isoformat(),
                "updated_at": record.updated_at.isoformat()
            })
        
        return {
            "records": result,
            "total": len(result)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/import-status/{import_date}")
def check_import_status(
    import_date: str,
    db: Session = Depends(get_db)
):
    """æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„å¯¼å…¥çŠ¶æ€"""
    try:
        try:
            query_date = datetime.strptime(import_date, '%Y-%m-%d').date()
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD")
        
        records = db.query(DataImportRecord).filter(
            DataImportRecord.import_date == query_date
        ).all()
        
        status = {
            "import_date": import_date,
            "csv_imported": False,
            "txt_imported": False,
            "csv_record": None,
            "txt_record": None
        }
        
        for record in records:
            record_data = {
                "id": record.id,
                "file_name": record.file_name,
                "imported_records": record.imported_records,
                "skipped_records": record.skipped_records,
                "import_status": record.import_status,
                "error_message": record.error_message,
                "created_at": record.created_at.isoformat(),
                "updated_at": record.updated_at.isoformat()
            }
            
            if record.import_type == 'csv':
                status["csv_imported"] = True
                status["csv_record"] = record_data
            elif record.import_type == 'txt':
                status["txt_imported"] = True
                status["txt_record"] = record_data
        
        return status
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.post("/import-daily-batch")
async def import_daily_batch(
    csv_file: UploadFile = File(..., description="CSVæ–‡ä»¶ - åŒ…å«è‚¡ç¥¨ä»£ç ã€åç§°ã€è¡Œä¸šã€æ¦‚å¿µç­‰ä¿¡æ¯"),
    txt_file: UploadFile = File(..., description="TXTæ–‡ä»¶ - åŒ…å«è‚¡ç¥¨ä»£ç å’Œçƒ­åº¦æ•°æ®"),
    trade_date: Optional[str] = Form(None, description="äº¤æ˜“æ—¥æœŸ (YYYY-MM-DD)ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ä»æ–‡ä»¶åè§£æ"),
    allow_overwrite: bool = Form(False, description="æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ•°æ®"),
    db: Session = Depends(get_db)
):
    """
    æ¯æ—¥æ‰¹é‡å¯¼å…¥æ¥å£
    åŒæ—¶å¯¼å…¥CSVå’ŒTXTæ–‡ä»¶ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
    """
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not csv_file.filename or not csv_file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="ç¬¬ä¸€ä¸ªæ–‡ä»¶å¿…é¡»æ˜¯CSVæ ¼å¼")
    
    if not txt_file.filename or not txt_file.filename.endswith('.txt'):
        raise HTTPException(status_code=400, detail="ç¬¬äºŒä¸ªæ–‡ä»¶å¿…é¡»æ˜¯TXTæ ¼å¼")
    
    # éªŒè¯æ–‡ä»¶å¤§å°
    if csv_file.size and csv_file.size > 100 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="CSVæ–‡ä»¶ä¸èƒ½è¶…è¿‡100MB")
    
    if txt_file.size and txt_file.size > 50 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="TXTæ–‡ä»¶ä¸èƒ½è¶…è¿‡50MB")
    
    try:
        # è§£æäº¤æ˜“æ—¥æœŸ
        parsed_trade_date = None
        if trade_date:
            try:
                parsed_trade_date = datetime.strptime(trade_date, '%Y-%m-%d').date()
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        csv_content = await csv_file.read()
        txt_content = await txt_file.read()
        
        # éªŒè¯æ–‡ä»¶å†…å®¹ä¸ä¸ºç©º
        if not csv_content:
            raise HTTPException(status_code=400, detail="CSVæ–‡ä»¶å†…å®¹ä¸ºç©º")
        
        if not txt_content:
            raise HTTPException(status_code=400, detail="TXTæ–‡ä»¶å†…å®¹ä¸ºç©º")
        
        print(f"ğŸ“„ å¼€å§‹æ‰¹é‡å¯¼å…¥: CSV({csv_file.filename}, {len(csv_content)} bytes) + TXT({txt_file.filename}, {len(txt_content)} bytes)")
        
        # ä½¿ç”¨æ•°æ®å¯¼å…¥æœåŠ¡å¤„ç†æ‰¹é‡å¯¼å…¥
        import_service = DataImportService(db)
        result = await import_service.import_daily_batch(
            csv_content, csv_file.filename,
            txt_content, txt_file.filename,
            parsed_trade_date, allow_overwrite
        )
        
        print(f"âœ… æ‰¹é‡å¯¼å…¥å®Œæˆ: {result['message']}")
        
        return {
            "message": result["message"],
            "success": result["success"],
            "trade_date": result["trade_date"],
            "csv_file": csv_file.filename,
            "txt_file": txt_file.filename,
            "csv_result": result["csv_result"],
            "txt_result": result["txt_result"],
            "overwrite": allow_overwrite
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¯¼å…¥å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {str(e)}")


@router.get("/check-completeness/{trade_date}")
def check_import_completeness(
    trade_date: str,
    db: Session = Depends(get_db)
):
    """
    æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®å¯¼å…¥å®Œæ•´æ€§
    è¿”å›CSVå’ŒTXTæ˜¯å¦éƒ½å·²å¯¼å…¥ï¼Œæ˜¯å¦å¯ä»¥è¿›è¡Œåˆ†æ
    """
    try:
        try:
            parsed_date = datetime.strptime(trade_date, '%Y-%m-%d').date()
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD")
        
        import_service = DataImportService(db)
        result = import_service.check_daily_import_completeness(parsed_date)
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å®Œæ•´æ€§å¤±è´¥: {str(e)}")


@router.post("/auto-extract-date")
async def auto_extract_date_from_files(
    csv_file: UploadFile = File(...),
    txt_file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    ä»æ–‡ä»¶åè‡ªåŠ¨æå–æ—¥æœŸä¿¡æ¯
    ç”¨äºå‰ç«¯é¢„è§ˆå°†è¦å¯¼å…¥çš„æ—¥æœŸ
    """
    try:
        import_service = DataImportService(db)
        
        # ä»CSVæ–‡ä»¶åæå–æ—¥æœŸ
        csv_date = import_service._extract_date_from_filename(csv_file.filename or "")
        txt_date = import_service._extract_date_from_filename(txt_file.filename or "")
        
        # é€‰æ‹©æœ€åˆé€‚çš„æ—¥æœŸ
        extracted_date = csv_date or txt_date or date.today()
        
        # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦å·²å¯¼å…¥æ•°æ®
        completeness = import_service.check_daily_import_completeness(extracted_date)
        
        return {
            "csv_filename": csv_file.filename,
            "txt_filename": txt_file.filename,
            "csv_extracted_date": csv_date.isoformat() if csv_date else None,
            "txt_extracted_date": txt_date.isoformat() if txt_date else None,
            "recommended_date": extracted_date.isoformat(),
            "already_imported": completeness
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ—¥æœŸæå–å¤±è´¥: {str(e)}")