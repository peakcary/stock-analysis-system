"""
æ•°æ®å¯¼å…¥æœåŠ¡
"""

import pandas as pd
import io
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session
from app.models import Stock, Concept, StockConcept, DailyStockData, DataImportRecord
from datetime import datetime, date


class DataImportService:
    """æ•°æ®å¯¼å…¥æœåŠ¡ç±»"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def check_existing_import(self, import_date: date, import_type: str, file_name: str) -> Optional[DataImportRecord]:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸå’Œç±»å‹çš„å¯¼å…¥è®°å½•"""
        return self.db.query(DataImportRecord).filter(
            DataImportRecord.import_date == import_date,
            DataImportRecord.import_type == import_type,
            DataImportRecord.file_name == file_name
        ).first()
    
    def create_import_record(self, import_date: date, import_type: str, file_name: str, 
                           imported_records: int = 0, skipped_records: int = 0,
                           import_status: str = 'success', error_message: str = None) -> DataImportRecord:
        """åˆ›å»ºå¯¼å…¥è®°å½•"""
        record = DataImportRecord(
            import_date=import_date,
            import_type=import_type,
            file_name=file_name,
            imported_records=imported_records,
            skipped_records=skipped_records,
            import_status=import_status,
            error_message=error_message
        )
        self.db.add(record)
        return record
    
    def update_import_record(self, record: DataImportRecord, imported_records: int = 0, 
                           skipped_records: int = 0, import_status: str = 'success', 
                           error_message: str = None):
        """æ›´æ–°å¯¼å…¥è®°å½•"""
        record.imported_records = imported_records
        record.skipped_records = skipped_records
        record.import_status = import_status
        record.error_message = error_message
        record.updated_at = datetime.now()
    
    async def import_csv_data(self, content: bytes, filename: str, allow_overwrite: bool = False, trade_date: date = None) -> Dict[str, Any]:
        """
        å¯¼å…¥CSVæ ¼å¼çš„è‚¡ç¥¨æ•°æ®
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. è‹±æ–‡æ ¼å¼: stock_code,stock_name,concept,industry,date,price,turnover_rate,net_inflow
        2. ä¸­æ–‡æ ¼å¼: è‚¡ç¥¨ä»£ç ,è‚¡ç¥¨åç§°,å…¨éƒ¨é¡µæ•°,çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°,ä»·æ ¼,è¡Œä¸š,æ¦‚å¿µ,æ¢æ‰‹,å‡€æµå…¥
        """
        # å¦‚æœæ²¡æœ‰æä¾›äº¤æ˜“æ—¥æœŸï¼Œä»æ–‡ä»¶åå°è¯•è§£ææ—¥æœŸ
        if not trade_date:
            import_date = self._extract_date_from_filename(filename)
            if not import_date:
                import_date = date.today()
        else:
            import_date = trade_date
        
        import_type = 'csv'
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¯¼å…¥è¿‡
            existing_record = self.check_existing_import(import_date, import_type, filename)
            if existing_record and not allow_overwrite:
                return {
                    "message": "ä»Šæ—¥å·²å¯¼å…¥è¯¥æ–‡ä»¶ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®allow_overwrite=True",
                    "imported_records": existing_record.imported_records,
                    "skipped_records": existing_record.skipped_records,
                    "import_date": import_date.isoformat(),
                    "already_exists": True
                }
            
            # è§£æCSVå†…å®¹
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8')
            
            # æ£€æµ‹CSVæ ¼å¼å¹¶è¿›è¡Œåˆ—åæ˜ å°„
            df = self._normalize_csv_columns(df)
            
            imported_records = 0
            skipped_records = 0
            errors = []
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'new_stocks': 0,
                'updated_stocks': 0,
                'new_concepts': 0,
                'new_relations': 0,
                'updated_daily_data': 0,
                'new_daily_data': 0
            }
            
            # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['stock_code', 'stock_name', 'concept']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise Exception(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}")
            
            # å¦‚æœæ˜¯è¦†ç›–å¯¼å…¥ï¼Œå…ˆåˆ é™¤ç›¸å…³æ•°æ®
            if allow_overwrite and existing_record:
                # è·å–CSVæ–‡ä»¶ä¸­æ¶‰åŠçš„è‚¡ç¥¨ä»£ç 
                stock_codes_in_csv = set(str(row['stock_code']).strip() for _, row in df.iterrows() 
                                       if pd.notna(row.get('stock_code')))
                
                # åªåˆ é™¤CSVä¸­æ¶‰åŠçš„è‚¡ç¥¨åœ¨è¯¥æ—¥æœŸçš„æ•°æ®
                if stock_codes_in_csv:
                    # å…ˆè·å–éœ€è¦åˆ é™¤çš„è‚¡ç¥¨ID
                    stock_objects = self.db.query(Stock).filter(
                        Stock.stock_code.in_(stock_codes_in_csv)
                    ).all()
                    
                    stock_ids = [stock.id for stock in stock_objects]
                    
                    if stock_ids:
                        # ä½¿ç”¨è‚¡ç¥¨IDç›´æ¥åˆ é™¤ï¼Œé¿å…joinæ“ä½œ
                        deleted_count = self.db.query(DailyStockData).filter(
                            DailyStockData.trade_date == import_date,
                            DailyStockData.stock_id.in_(stock_ids)
                        ).delete(synchronize_session=False)
                        self.db.flush()
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(stock_codes_in_csv)} åªè‚¡ç¥¨åœ¨ {import_date} çš„ {deleted_count} æ¡æ•°æ®è®°å½•")
                
                # æ¸…ç†å¯èƒ½è¿‡æ—¶çš„è‚¡ç¥¨æ¦‚å¿µå…³è”ï¼ˆå¯é€‰ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä¸åˆ é™¤æ¦‚å¿µå…³è”ï¼Œå› ä¸ºå…¶ä»–æ—¥æœŸå¯èƒ½ä»ç„¶æœ‰æ•ˆ
            
            # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„è‚¡ç¥¨-æ—¥æœŸç»„åˆï¼Œé¿å…é‡å¤æ’å…¥DailyStockData
            processed_stock_dates = set()
            
            # ç”¨äºè·Ÿè¸ªæœ¬æ¬¡CSVä¸­çš„è‚¡ç¥¨å’Œæ¦‚å¿µå…³ç³»
            csv_stock_concepts = {}  # {stock_code: [concept_names]}
            csv_stocks_info = {}     # {stock_code: {'name': ..., 'industry': ...}}
            
            # ç¬¬ä¸€éï¼šæ”¶é›†CSVä¸­çš„æ‰€æœ‰è‚¡ç¥¨å’Œæ¦‚å¿µä¿¡æ¯
            print(f"ğŸ“Š ç¬¬ä¸€æ­¥ï¼šåˆ†æCSVæ–‡ä»¶ä¸­çš„è‚¡ç¥¨å’Œæ¦‚å¿µå…³ç³»...")
            for index, row in df.iterrows():
                try:
                    if pd.isna(row.get('stock_code')) or pd.isna(row.get('stock_name')):
                        continue
                    
                    stock_code = str(row['stock_code']).strip()
                    stock_name = str(row['stock_name']).strip()
                    industry = str(row.get('industry', '')).strip() if not pd.isna(row.get('industry')) else ''
                    concept_name = str(row['concept']).strip()
                    
                    if not stock_code or not stock_name or not concept_name:
                        continue
                    
                    # æ”¶é›†è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆæ€»æ˜¯ä½¿ç”¨æœ€æ–°çš„ï¼‰
                    csv_stocks_info[stock_code] = {
                        'name': stock_name,
                        'industry': industry
                    }
                    
                    # æ”¶é›†æ¦‚å¿µå…³ç³»
                    if stock_code not in csv_stock_concepts:
                        csv_stock_concepts[stock_code] = set()
                    csv_stock_concepts[stock_code].add(concept_name)
                    
                except Exception as e:
                    continue
            
            print(f"ğŸ“ˆ CSVä¸­åŒ…å« {len(csv_stocks_info)} åªè‚¡ç¥¨ï¼Œ{sum(len(concepts) for concepts in csv_stock_concepts.values())} ä¸ªè‚¡ç¥¨-æ¦‚å¿µå…³ç³»")
            
            # ç¬¬äºŒéï¼šå¤„ç†è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å’Œæ¦‚å¿µå…³ç³»
            for index, row in df.iterrows():
                try:
                    # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆè¡Œ
                    if pd.isna(row.get('stock_code')) or pd.isna(row.get('stock_name')):
                        skipped_records += 1
                        continue
                    
                    # å¤„ç†è‚¡ç¥¨ä¿¡æ¯
                    stock_code = str(row['stock_code']).strip()
                    stock_name = str(row['stock_name']).strip()
                    industry = str(row.get('industry', '')).strip() if not pd.isna(row.get('industry')) else ''
                    concept_name = str(row['concept']).strip()
                    
                    # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                    if not stock_code or not stock_name or not concept_name:
                        skipped_records += 1
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºè½¬å€ºï¼ˆä»¥1å¼€å¤´çš„6ä½ä»£ç ï¼‰
                    is_convertible_bond = (
                        len(stock_code) == 6 and 
                        stock_code.startswith('1') and 
                        stock_code.isdigit()
                    )
                    
                    # è·å–æˆ–åˆ›å»ºè‚¡ç¥¨è®°å½•
                    stock = self.db.query(Stock).filter(
                        Stock.stock_code == stock_code
                    ).first()
                    
                    if not stock:
                        # åˆ›å»ºæ–°è‚¡ç¥¨
                        stock = Stock(
                            stock_code=stock_code,
                            stock_name=stock_name,
                            industry=industry if industry else None,
                            is_convertible_bond=is_convertible_bond
                        )
                        self.db.add(stock)
                        self.db.flush()  # è·å–ID
                        stats['new_stocks'] += 1
                        print(f"âœ¨ åˆ›å»ºæ–°è‚¡ç¥¨: {stock_code} - {stock_name}")
                    else:
                        # æ›´æ–°ç°æœ‰è‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯ï¼ˆæ€»æ˜¯ä½¿ç”¨æœ€æ–°CSVçš„æ•°æ®ï¼‰
                        updated_fields = []
                        if stock.stock_name != stock_name:
                            stock.stock_name = stock_name
                            updated_fields.append(f"åç§°: {stock_name}")
                        if stock.industry != (industry if industry else None):
                            stock.industry = industry if industry else None
                            updated_fields.append(f"è¡Œä¸š: {industry or 'æ— '}")
                        if updated_fields:
                            stats['updated_stocks'] += 1
                            print(f"ğŸ”„ æ›´æ–°è‚¡ç¥¨ {stock_code}: {', '.join(updated_fields)}")
                    
                    # è·å–æˆ–åˆ›å»ºæ¦‚å¿µè®°å½•
                    concept = self.db.query(Concept).filter(
                        Concept.concept_name == concept_name
                    ).first()
                    
                    if not concept:
                        concept = Concept(concept_name=concept_name)
                        self.db.add(concept)
                        self.db.flush()  # è·å–ID
                        stats['new_concepts'] += 1
                        print(f"âœ¨ åˆ›å»ºæ–°æ¦‚å¿µ: {concept_name}")
                    
                    # åˆ›å»ºè‚¡ç¥¨æ¦‚å¿µå…³è”ï¼ˆå¢é‡æ¨¡å¼ï¼šåªæ·»åŠ ï¼Œä¸åˆ é™¤æ—§å…³ç³»ï¼‰
                    stock_concept = self.db.query(StockConcept).filter(
                        StockConcept.stock_id == stock.id,
                        StockConcept.concept_id == concept.id
                    ).first()
                    
                    if not stock_concept:
                        stock_concept = StockConcept(
                            stock_id=stock.id,
                            concept_id=concept.id
                        )
                        self.db.add(stock_concept)
                        stats['new_relations'] += 1
                        print(f"ğŸ”— æ·»åŠ å…³è”: {stock_code} -> {concept_name}")
                    
                    # å¤„ç†æ¯æ—¥æ•°æ®ï¼ˆç°åœ¨æ€»æ˜¯æœ‰æ—¥æœŸä¿¡æ¯ï¼‰
                    if 'date' in df.columns:
                        trade_date = pd.to_datetime(row['date']).date()
                        stock_date_key = (stock.id, trade_date)
                        
                        # åªæœ‰åœ¨æœªå¤„ç†è¿‡è¯¥è‚¡ç¥¨-æ—¥æœŸç»„åˆæ—¶æ‰å¤„ç†DailyStockData
                        if stock_date_key not in processed_stock_dates:
                            price = float(row.get('price', 0)) if pd.notna(row.get('price')) else 0
                            turnover_rate = float(row.get('turnover_rate', 0)) if pd.notna(row.get('turnover_rate')) else 0
                            net_inflow = float(row.get('net_inflow', 0)) if pd.notna(row.get('net_inflow')) else 0
                            pages_count = int(row.get('pages_count', 0)) if pd.notna(row.get('pages_count')) else 0
                            total_reads = int(row.get('total_reads', 0)) if pd.notna(row.get('total_reads')) else 0
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ—¥æœŸçš„æ•°æ®
                            existing_data = self.db.query(DailyStockData).filter(
                                DailyStockData.stock_id == stock.id,
                                DailyStockData.trade_date == trade_date
                            ).first()
                            
                            if existing_data:
                                # æ›´æ–°ç°æœ‰è®°å½•
                                existing_data.price = price
                                existing_data.turnover_rate = turnover_rate
                                existing_data.net_inflow = net_inflow
                                existing_data.pages_count = pages_count
                                existing_data.total_reads = total_reads
                                stats['updated_daily_data'] += 1
                            else:
                                # åˆ›å»ºæ–°è®°å½•
                                daily_data = DailyStockData(
                                    stock_id=stock.id,
                                    trade_date=trade_date,
                                    price=price,
                                    turnover_rate=turnover_rate,
                                    net_inflow=net_inflow,
                                    pages_count=pages_count,
                                    total_reads=total_reads,
                                    heat_value=0  # é»˜è®¤çƒ­åº¦å€¼ä¸º0
                                )
                                self.db.add(daily_data)
                                stats['new_daily_data'] += 1
                            
                            # æ ‡è®°è¯¥è‚¡ç¥¨-æ—¥æœŸç»„åˆå·²å¤„ç†
                            processed_stock_dates.add(stock_date_key)
                    
                    imported_records += 1
                    
                except Exception as e:
                    skipped_records += 1
                    errors.append(f"ç¬¬{index+1}è¡Œ: {str(e)}")
                    continue
            
            # åˆ›å»ºæˆ–æ›´æ–°å¯¼å…¥è®°å½•
            if existing_record and allow_overwrite:
                self.update_import_record(
                    existing_record, imported_records, skipped_records, 
                    'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None  # åªä¿å­˜å‰5ä¸ªé”™è¯¯
                )
            else:
                self.create_import_record(
                    import_date, import_type, filename, imported_records, 
                    skipped_records, 'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            
            # æ‰“å°å¯¼å…¥æ€»ç»“
            print(f"\nğŸ“ˆ CSVå¯¼å…¥å®Œæˆæ€»ç»“:")
            print(f"   ğŸ“‹ æ–‡ä»¶å: {filename}")
            print(f"   ğŸ“… å¯¼å…¥æ—¥æœŸ: {import_date}")
            print(f"   ğŸ“Š å¤„ç†è®°å½•: {imported_records} æˆåŠŸ, {skipped_records} è·³è¿‡")
            print(f"   ğŸ¢ è‚¡ç¥¨ä¿¡æ¯: {stats['new_stocks']} æ–°å¢, {stats['updated_stocks']} æ›´æ–°")
            print(f"   ğŸ·ï¸  æ¦‚å¿µä¿¡æ¯: {stats['new_concepts']} æ–°å¢æ¦‚å¿µ")
            print(f"   ğŸ”— å…³è”å…³ç³»: {stats['new_relations']} æ–°å¢å…³è”")
            print(f"   ğŸ“ˆ æ¯æ—¥æ•°æ®: {stats['new_daily_data']} æ–°å¢, {stats['updated_daily_data']} æ›´æ–°")
            if errors:
                print(f"   âš ï¸  é”™è¯¯æ•°é‡: {len(errors)} ä¸ª")
            print(f"   âœ… å¯¼å…¥çŠ¶æ€: {'å®Œå…¨æˆåŠŸ' if not errors else 'éƒ¨åˆ†æˆåŠŸ'}")
            
            # æäº¤äº‹åŠ¡
            self.db.commit()
            
            return {
                "imported_records": imported_records,
                "skipped_records": skipped_records,
                "errors": errors,
                "import_date": import_date.isoformat(),
                "overwrite": allow_overwrite and existing_record is not None,
                "stats": stats
            }
            
        except Exception as e:
            self.db.rollback()
            raise Exception(f"CSVè§£æå¤±è´¥: {str(e)}")
    
    async def import_txt_data(self, content: bytes, filename: str, allow_overwrite: bool = False, trade_date: date = None) -> Dict[str, Any]:
        """
        å¯¼å…¥TXTæ ¼å¼çš„æ¯æ—¥äº¤æ˜“æ•°æ®
        
        TXTæ–‡ä»¶ç‰¹ç‚¹ï¼š
        1. åŒ…å«æ¯æ—¥äº¤æ˜“æ•°æ®ï¼Œæ¯è¡Œæ ¼å¼: è‚¡ç¥¨ä»£ç \tæ—¥æœŸ\tçƒ­åº¦å€¼
        2. æ”¯æŒåŸºäºæ—¥æœŸçš„å®Œå…¨è¦†ç›–å¯¼å…¥
        3. æ—¥æœŸæ¥æºï¼šä¼˜å…ˆä½¿ç”¨æ–‡ä»¶å†…å®¹ä¸­çš„æ—¥æœŸï¼Œå…¶æ¬¡æ˜¯å‚æ•°ï¼Œæœ€åæ˜¯æ–‡ä»¶å
        
        å¯¼å…¥ç­–ç•¥ï¼š
        - å¦‚æœæ˜¯åŒä¸€æ—¥æœŸçš„é‡å¤å¯¼å…¥ï¼Œå®Œå…¨è¦†ç›–è¯¥æ—¥æœŸçš„æ‰€æœ‰æ•°æ®
        - æ”¯æŒæ•°æ®çº æ­£ï¼šé‡æ–°å¯¼å…¥å¯ä»¥è¦†ç›–ä¹‹å‰é”™è¯¯çš„æ•°æ®
        """
        
        # ç¬¬ä¸€æ­¥ï¼šé¢„è§£æTXTå†…å®¹ä»¥ç¡®å®šæ—¥æœŸå’Œæ•°æ®èŒƒå›´
        text_content = content.decode('utf-8')
        lines = text_content.strip().split('\n')
        
        # ä»æ–‡ä»¶å†…å®¹ä¸­æå–æ—¥æœŸ
        detected_dates = set()
        valid_lines = []
        
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line:
                continue
                
            parts = line.split('\t') if '\t' in line else line.split()
            if len(parts) >= 3:
                try:
                    # å°è¯•è§£ææ—¥æœŸ (ç¬¬äºŒä¸ªå­—æ®µåº”è¯¥æ˜¯æ—¥æœŸ)
                    date_str = parts[1].strip()
                    parsed_date = self._parse_date_from_string(date_str)
                    if parsed_date:
                        detected_dates.add(parsed_date)
                        valid_lines.append((line_num, line, parsed_date))
                except:
                    continue
        
        # ç¡®å®šå¯¼å…¥çš„ç›®æ ‡æ—¥æœŸ
        if trade_date:
            target_date = trade_date
            print(f"ğŸ“… ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {target_date}")
        elif len(detected_dates) == 1:
            target_date = list(detected_dates)[0]
            print(f"ğŸ“… ä»æ–‡ä»¶å†…å®¹æ£€æµ‹åˆ°æ—¥æœŸ: {target_date}")
        elif len(detected_dates) > 1:
            # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªæ—¥æœŸï¼Œä½¿ç”¨æœ€å¸¸è§çš„æ—¥æœŸ
            from collections import Counter
            date_counts = Counter([line[2] for line in valid_lines])
            target_date = date_counts.most_common(1)[0][0]
            print(f"ğŸ“… æ£€æµ‹åˆ°å¤šä¸ªæ—¥æœŸï¼Œä½¿ç”¨æœ€å¸¸è§çš„: {target_date} (å‡ºç°{date_counts[target_date]}æ¬¡)")
        else:
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            target_date = self._extract_date_from_filename(filename)
            if not target_date:
                target_date = date.today()
            print(f"ğŸ“… ä»æ–‡ä»¶åæå–æ—¥æœŸ: {target_date}")
        
        import_type = 'txt'
        
        print(f"ğŸ“Š TXTæ–‡ä»¶é¢„åˆ†æ:")
        print(f"   ğŸ“‹ æ–‡ä»¶å: {filename}")
        print(f"   ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")  
        print(f"   ğŸ“ æœ‰æ•ˆè¡Œæ•°: {len(valid_lines)}")
        print(f"   ğŸ” æ£€æµ‹åˆ°æ—¥æœŸ: {sorted(detected_dates)}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¯¼å…¥è¿‡è¯¥æ—¥æœŸçš„æ•°æ®
            existing_record = self.check_existing_import(target_date, import_type, filename)
            
            # TXTæ–‡ä»¶æ”¯æŒé‡å¤å¯¼å…¥å’Œè¦†ç›–ï¼ˆç”¨äºæ•°æ®çº æ­£ï¼‰
            if existing_record and not allow_overwrite:
                print(f"âš ï¸  è¯¥æ—¥æœŸ {target_date} å·²æœ‰TXTæ•°æ®ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®allow_overwrite=True")
                return {
                    "message": f"è¯¥æ—¥æœŸ {target_date} å·²æœ‰æ•°æ®ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®allow_overwrite=True",
                    "imported_records": existing_record.imported_records,
                    "skipped_records": existing_record.skipped_records,
                    "import_date": target_date.isoformat(),
                    "already_exists": True
                }
            
            imported_records = 0
            skipped_records = 0
            errors = []
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'target_date': target_date.isoformat(),
                'deleted_records': 0,
                'updated_records': 0,
                'new_records': 0,
                'error_records': 0
            }
            
            # æ ¸å¿ƒç­–ç•¥ï¼šåŸºäºæ—¥æœŸçš„å®Œå…¨è¦†ç›–
            # 1. æ”¶é›†è¦å¤„ç†çš„è‚¡ç¥¨ä»£ç 
            txt_stock_codes = set()
            for line_num, line, line_date in valid_lines:
                if line_date != target_date:
                    continue
                
                parts = line.split('\t') if '\t' in line else line.split()
                if len(parts) >= 3:
                    stock_code_with_prefix = parts[0].strip()
                    stock_code = self._normalize_stock_code(stock_code_with_prefix)
                    if stock_code.isdigit() and len(stock_code) == 6:
                        txt_stock_codes.add(stock_code)
            
            print(f"ğŸ”„ å‡†å¤‡å¤„ç† {len(txt_stock_codes)} åªè‚¡ç¥¨åœ¨ {target_date} çš„çƒ­åº¦æ•°æ®")
            
            # 2. å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼æˆ–å·²å­˜åœ¨æ•°æ®ï¼Œå…ˆåˆ é™¤è¯¥æ—¥æœŸçš„ç›¸å…³æ•°æ®
            if allow_overwrite or existing_record:
                if txt_stock_codes:
                    # å…ˆè·å–è¦åˆ é™¤çš„è‚¡ç¥¨ID
                    stock_objects = self.db.query(Stock).filter(
                        Stock.stock_code.in_(txt_stock_codes)
                    ).all()
                    
                    stock_ids = [stock.id for stock in stock_objects]
                    
                    if stock_ids:
                        # åˆ é™¤è¯¥æ—¥æœŸä¸‹è¿™äº›è‚¡ç¥¨çš„ç°æœ‰æ•°æ®
                        deleted_count = self.db.query(DailyStockData).filter(
                            DailyStockData.trade_date == target_date,
                            DailyStockData.stock_id.in_(stock_ids)
                        ).delete(synchronize_session=False)
                        
                        stats['deleted_records'] = deleted_count
                        self.db.flush()
                        
                        if deleted_count > 0:
                            print(f"ğŸ—‘ï¸  åˆ é™¤äº† {deleted_count} æ¡ {target_date} çš„æ—§æ•°æ®ï¼Œå‡†å¤‡å¯¼å…¥æ–°æ•°æ®")
            
            # 3. å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            print(f"ğŸ“ å¼€å§‹å¤„ç†TXTæ•°æ®...")
            
            for line_num, line, line_date in valid_lines:
                try:
                    # åªå¤„ç†ç›®æ ‡æ—¥æœŸçš„æ•°æ®
                    if line_date != target_date:
                        skipped_records += 1
                        continue
                        
                    # åˆ†å‰²æ•°æ®éƒ¨åˆ† - æ”¯æŒåˆ¶è¡¨ç¬¦åˆ†éš”
                    parts = line.split('\t') if '\t' in line else line.split()
                    if len(parts) < 3:
                        skipped_records += 1
                        errors.append(f"ç¬¬{line_num}è¡Œ: æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦è‡³å°‘3ä¸ªå­—æ®µ")
                        continue
                    
                    # è§£æå­—æ®µï¼šè‚¡ç¥¨ä»£ç ã€æ—¥æœŸã€çƒ­åº¦å€¼
                    stock_code_with_prefix = parts[0].strip()
                    date_str = parts[1].strip()
                    heat_value_str = parts[2].strip()
                    
                    # è§£æçƒ­åº¦å€¼
                    try:
                        heat_value = float(heat_value_str)
                    except ValueError:
                        skipped_records += 1
                        errors.append(f"ç¬¬{line_num}è¡Œ: æ— æ³•è§£æçƒ­åº¦å€¼ '{heat_value_str}'")
                        stats['error_records'] += 1
                        continue
                    
                    # å¤„ç†è‚¡ç¥¨ä»£ç  - å»æ‰SH/SZå‰ç¼€
                    stock_code = self._normalize_stock_code(stock_code_with_prefix)
                    
                    # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆ6ä½æ•°å­—ï¼‰
                    if not stock_code or not stock_code.isdigit() or len(stock_code) != 6:
                        skipped_records += 1
                        errors.append(f"ç¬¬{line_num}è¡Œ: è‚¡ç¥¨ä»£ç æ ¼å¼æ— æ•ˆ {stock_code_with_prefix} -> {stock_code}")
                        stats['error_records'] += 1
                        continue
                    
                    # æŸ¥æ‰¾è‚¡ç¥¨è®°å½•
                    stock = self.db.query(Stock).filter(
                        Stock.stock_code == stock_code
                    ).first()
                    
                    if not stock:
                        # ä¸ºä¸å­˜åœ¨çš„è‚¡ç¥¨åˆ›å»ºåŸºç¡€è®°å½•
                        is_convertible_bond = (
                            len(stock_code) == 6 and 
                            stock_code.startswith('1') and 
                            stock_code.isdigit()
                        )
                        
                        stock = Stock(
                            stock_code=stock_code,
                            stock_name=f"è‚¡ç¥¨{stock_code}",  # ä¸´æ—¶åç§°
                            is_convertible_bond=is_convertible_bond
                        )
                        self.db.add(stock)
                        self.db.flush()  # è·å–ID
                        print(f"ğŸ’« è‡ªåŠ¨åˆ›å»ºè‚¡ç¥¨è®°å½•: {stock_code}")
                    
                    # åˆ›å»ºæ¯æ—¥æ•°æ®è®°å½•ï¼ˆå› ä¸ºä¹‹å‰å·²åˆ é™¤ï¼Œè¿™é‡Œéƒ½æ˜¯æ–°å»ºï¼‰
                    daily_data = DailyStockData(
                        stock_id=stock.id,
                        trade_date=target_date,
                        heat_value=heat_value,
                        # å…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼
                        price=0,
                        turnover_rate=0,
                        net_inflow=0,
                        pages_count=0,
                        total_reads=0
                    )
                    self.db.add(daily_data)
                    stats['new_records'] += 1
                    imported_records += 1
                    
                    if imported_records % 100 == 0:
                        print(f"   ... å·²å¤„ç† {imported_records} æ¡è®°å½•")
                    
                except Exception as e:
                    skipped_records += 1
                    stats['error_records'] += 1
                    errors.append(f"ç¬¬{line_num}è¡Œ: {str(e)}")
                    continue
            
            # æ‰“å°å¯¼å…¥æ€»ç»“
            print(f"\nğŸ“ˆ TXTå¯¼å…¥å®Œæˆæ€»ç»“:")
            print(f"   ğŸ“‹ æ–‡ä»¶å: {filename}")
            print(f"   ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
            print(f"   ğŸ“Š å¤„ç†è®°å½•: {imported_records} æˆåŠŸ, {skipped_records} è·³è¿‡")
            print(f"   ğŸ—‘ï¸  åˆ é™¤è®°å½•: {stats['deleted_records']} æ¡æ—§æ•°æ®")
            print(f"   âœ¨ æ–°å¢è®°å½•: {stats['new_records']} æ¡çƒ­åº¦æ•°æ®")
            print(f"   âŒ é”™è¯¯è®°å½•: {stats['error_records']} æ¡")
            if errors:
                print(f"   âš ï¸  é”™è¯¯è¯¦æƒ…: æ˜¾ç¤ºå‰3ä¸ª")
                for error in errors[:3]:
                    print(f"      - {error}")
            print(f"   âœ… å¯¼å…¥çŠ¶æ€: {'å®Œå…¨æˆåŠŸ' if not errors else 'éƒ¨åˆ†æˆåŠŸ'}")
            print(f"   ğŸ”„ è¦†ç›–æ¨¡å¼: {'æ˜¯' if (allow_overwrite or existing_record) else 'å¦'}")
            
            # åˆ›å»ºæˆ–æ›´æ–°å¯¼å…¥è®°å½•
            if existing_record and allow_overwrite:
                self.update_import_record(
                    existing_record, imported_records, skipped_records,
                    'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            else:
                self.create_import_record(
                    target_date, import_type, filename, imported_records,
                    skipped_records, 'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            
            # æäº¤äº‹åŠ¡
            self.db.commit()
            
            return {
                "imported_records": imported_records,
                "skipped_records": skipped_records,
                "errors": errors,
                "import_date": target_date.isoformat(),
                "overwrite": allow_overwrite or existing_record is not None,
                "stats": stats
            }
            
        except Exception as e:
            self.db.rollback()
            raise Exception(f"TXTè§£æå¤±è´¥: {str(e)}")
    
    def _extract_date_from_filename(self, filename: str) -> date:
        """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        import re
        
        # æ”¯æŒçš„æ—¥æœŸæ ¼å¼æ¨¡å¼
        patterns = [
            (r'(\d{4}-\d{2}-\d{2})', '%Y-%m-%d'),  # 2025-08-28
            (r'(\d{4}_\d{2}_\d{2})', '%Y_%m_%d'),  # 2025_08_28
            (r'(\d{8})', '%Y%m%d'),                # 20250828
            (r'(\d{4})(\d{2})(\d{2})', '%Y%m%d'),  # åˆ†ç»„å½¢å¼çš„20250828
            (r'(\d{2}-\d{2}-\d{4})', '%m-%d-%Y'),  # MM-DD-YYYY
            (r'(\d{2}/\d{2}/\d{4})', '%m/%d/%Y'),  # MM/DD/YYYY
        ]
        
        for pattern, date_format in patterns:
            match = re.search(pattern, filename)
            if match:
                if len(match.groups()) == 1:
                    date_str = match.group(1)
                else:
                    # å¤„ç†åˆ†ç»„åŒ¹é…ï¼Œå¦‚(\d{4})(\d{2})(\d{2})
                    date_str = ''.join(match.groups())
                    
                try:
                    return datetime.strptime(date_str, date_format).date()
                except ValueError:
                    continue
        
        return None
    
    def _normalize_csv_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–CSVåˆ—åï¼Œæ”¯æŒä¸­è‹±æ–‡æ ¼å¼è‡ªåŠ¨è½¬æ¢
        """
        # ä¸­æ–‡åˆ°è‹±æ–‡çš„åˆ—åæ˜ å°„
        chinese_to_english = {
            'è‚¡ç¥¨ä»£ç ': 'stock_code',
            'è‚¡ç¥¨åç§°': 'stock_name',
            'æ¦‚å¿µ': 'concept',
            'è¡Œä¸š': 'industry',
            'ä»·æ ¼': 'price',
            'æ¢æ‰‹': 'turnover_rate',
            'å‡€æµå…¥': 'net_inflow',
            'å…¨éƒ¨é¡µæ•°': 'pages_count',
            'çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°': 'total_reads'
        }
        
        # æ£€æµ‹æ˜¯å¦ä¸ºä¸­æ–‡æ ¼å¼
        columns = df.columns.tolist()
        is_chinese_format = any(col in chinese_to_english for col in columns)
        
        if is_chinese_format:
            # é‡å‘½ååˆ—
            df = df.rename(columns=chinese_to_english)
            
            # æ·»åŠ dateåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'date' not in df.columns:
                df['date'] = date.today().strftime('%Y-%m-%d')
            
            # å¤„ç†æ•°æ®ç±»å‹å’Œæ¸…ç†
            if 'industry' in df.columns:
                df['industry'] = df['industry'].replace('None', '').fillna('')
            
            if 'price' in df.columns:
                df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)
                
            if 'turnover_rate' in df.columns:
                df['turnover_rate'] = pd.to_numeric(df['turnover_rate'], errors='coerce').fillna(0)
                
            if 'net_inflow' in df.columns:
                df['net_inflow'] = pd.to_numeric(df['net_inflow'], errors='coerce').fillna(0)
        
        return df
    
    def _parse_date_from_string(self, date_str: str) -> date:
        """ä»å­—ç¬¦ä¸²ä¸­è§£ææ—¥æœŸï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        from datetime import datetime
        
        # æ”¯æŒçš„æ—¥æœŸæ ¼å¼
        formats = [
            '%Y-%m-%d',    # 2025-08-28
            '%Y/%m/%d',    # 2025/08/28
            '%Y%m%d',      # 20250828
            '%m/%d/%Y',    # 08/28/2025
            '%d/%m/%Y',    # 28/08/2025
            '%m-%d-%Y',    # 08-28-2025
            '%d-%m-%Y',    # 28-08-2025
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt).date()
            except ValueError:
                continue
        
        return None
    
    def _normalize_stock_code(self, stock_code_with_prefix: str) -> str:
        """
        è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ï¼Œå»æ‰SH/SZç­‰å‰ç¼€
        SH600000 -> 600000
        SZ000001 -> 000001
        600000 -> 600000
        """
        stock_code = stock_code_with_prefix.strip().upper()
        
        # å»æ‰å¸¸è§å‰ç¼€
        if stock_code.startswith('SH'):
            return stock_code[2:]
        elif stock_code.startswith('SZ'):
            return stock_code[2:]
        elif stock_code.startswith('BJ'):  # åŒ—äº¤æ‰€
            return stock_code[2:]
        elif stock_code.startswith('HK'):  # æ¸¯è‚¡
            return stock_code[2:]
        
        return stock_code
    
    async def import_daily_batch(self, csv_content: bytes, csv_filename: str, 
                                txt_content: bytes, txt_filename: str, 
                                trade_date: date = None, allow_overwrite: bool = False,
                                import_mode: str = "smart") -> Dict[str, Any]:
        """
        æ‰¹é‡å¯¼å…¥æ¯æ—¥æ•°æ®ï¼ˆCSV + TXTï¼‰
        ç¡®ä¿ä¸¤ä¸ªæ–‡ä»¶éƒ½æˆåŠŸå¯¼å…¥æ‰ç®—å®Œæˆ
        
        å‚æ•°:
            import_mode: å¯¼å…¥æ¨¡å¼
                - "smart": æ™ºèƒ½æ¨¡å¼ï¼Œè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¼å…¥
                - "skip": è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
                - "update": æ›´æ–°æ¨¡å¼ï¼Œåªæ›´æ–°å·²å­˜åœ¨çš„æ•°æ®
                - "overwrite": å®Œå…¨è¦†ç›–æ¨¡å¼
        """
        if not trade_date:
            # ä¼˜å…ˆä»CSVæ–‡ä»¶åè§£ææ—¥æœŸ
            trade_date = self._extract_date_from_filename(csv_filename)
            if not trade_date:
                trade_date = self._extract_date_from_filename(txt_filename)
            if not trade_date:
                trade_date = date.today()
        
        results = {
            "trade_date": trade_date.isoformat(),
            "csv_result": None,
            "txt_result": None,
            "success": False,
            "message": ""
        }
        
        try:
            # 1. å…ˆå¯¼å…¥CSVæ•°æ®ï¼ˆè‚¡ç¥¨åŸºç¡€ä¿¡æ¯å’Œæ¦‚å¿µï¼‰
            print(f"ğŸ”„ å¼€å§‹å¯¼å…¥CSVæ•°æ®: {csv_filename}")
            csv_result = await self.import_csv_data(csv_content, csv_filename, allow_overwrite)
            results["csv_result"] = csv_result
            
            if csv_result.get("already_exists") and not allow_overwrite:
                # CSVå·²å­˜åœ¨ï¼Œç»§ç»­æ£€æŸ¥TXTæ˜¯å¦ä¹Ÿå­˜åœ¨
                print(f"âš ï¸ CSVæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥TXTæ–‡ä»¶çŠ¶æ€...")
                txt_existing = self.check_existing_import(trade_date, 'txt', txt_filename)
                if txt_existing:
                    results["message"] = "CSVå’ŒTXTæ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼Œå¦‚éœ€é‡æ–°å¯¼å…¥è¯·è®¾ç½®allow_overwrite=True"
                    results["txt_result"] = {
                        "already_exists": True,
                        "imported_records": txt_existing.imported_records,
                        "skipped_records": txt_existing.skipped_records,
                        "import_date": trade_date.isoformat()
                    }
                    return results
                else:
                    # åªå¯¼å…¥TXTæ–‡ä»¶
                    print(f"ğŸ”„ CSVå·²å­˜åœ¨ï¼Œä»…å¯¼å…¥TXTæ•°æ®...")
                    txt_result = await self.import_txt_data(txt_content, txt_filename, allow_overwrite, trade_date)
                    results["txt_result"] = txt_result
                    
                    txt_success = not txt_result.get("already_exists", False) or txt_result.get("imported_records", 0) > 0
                    if txt_success:
                        results["success"] = True
                        results["message"] = f"âœ… CSVå·²å­˜åœ¨ï¼Œä»…å¯¼å…¥TXTæ•°æ®({txt_result.get('imported_records', 0)}æ¡)"
                    else:
                        results["message"] = f"âš ï¸ CSVå·²å­˜åœ¨ï¼ŒTXTå¯¼å…¥å¤±è´¥"
                    return results
            
            # 2. å†å¯¼å…¥TXTæ•°æ®ï¼ˆçƒ­åº¦æ•°æ®ï¼‰
            print(f"ğŸ”„ å¼€å§‹å¯¼å…¥TXTæ•°æ®: {txt_filename}")
            txt_result = await self.import_txt_data(txt_content, txt_filename, allow_overwrite, trade_date)
            results["txt_result"] = txt_result
            
            # 3. æ£€æŸ¥ä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦éƒ½æˆåŠŸå¯¼å…¥
            csv_success = not csv_result.get("already_exists", False) or csv_result.get("imported_records", 0) > 0
            txt_success = not txt_result.get("already_exists", False) or txt_result.get("imported_records", 0) > 0
            
            if csv_success and txt_success:
                results["success"] = True
                results["message"] = f"âœ… æˆåŠŸå¯¼å…¥ {trade_date} çš„æ•°æ®ï¼šCSV({csv_result.get('imported_records', 0)}æ¡) + TXT({txt_result.get('imported_records', 0)}æ¡)"
                
                # åˆ›å»ºæ‰¹é‡å¯¼å…¥è®°å½•
                batch_record = self.create_import_record(
                    trade_date, 'both', f"{csv_filename}+{txt_filename}",
                    csv_result.get('imported_records', 0) + txt_result.get('imported_records', 0),
                    csv_result.get('skipped_records', 0) + txt_result.get('skipped_records', 0),
                    'success'
                )
                self.db.commit()
                
                # è§¦å‘æ¯æ—¥åˆ†æè®¡ç®—
                try:
                    print(f"ğŸš€ è§¦å‘ {trade_date} çš„æ¯æ—¥åˆ†æè®¡ç®—...")
                    from app.services.ranking_calculator import RankingCalculatorService
                    ranking_service = RankingCalculatorService(self.db)
                    analysis_result = await ranking_service.trigger_full_analysis(trade_date)
                    print(f"âœ… åˆ†æè®¡ç®—å®Œæˆ: {analysis_result['message']}")
                except Exception as e:
                    print(f"âš ï¸ åˆ†æè®¡ç®—å¤±è´¥ï¼ˆä½†å¯¼å…¥æˆåŠŸï¼‰: {str(e)}")
                    # ä¸å½±å“å¯¼å…¥ç»“æœï¼Œåªè®°å½•è­¦å‘Š
                
            else:
                results["message"] = f"âš ï¸ éƒ¨åˆ†å¯¼å…¥å¤±è´¥ï¼šCSV({'æˆåŠŸ' if csv_success else 'å¤±è´¥'}) + TXT({'æˆåŠŸ' if txt_success else 'å¤±è´¥'})"
            
            return results
            
        except Exception as e:
            self.db.rollback()
            results["message"] = f"âŒ æ‰¹é‡å¯¼å…¥å¤±è´¥: {str(e)}"
            raise Exception(f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {str(e)}")
    
    def check_daily_import_completeness(self, trade_date: date) -> Dict[str, Any]:
        """
        æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„å¯¼å…¥å®Œæ•´æ€§
        ç¡®ä¿CSVå’ŒTXTæ•°æ®éƒ½å·²å¯¼å…¥
        """
        csv_records = self.db.query(DataImportRecord).filter(
            DataImportRecord.import_date == trade_date,
            DataImportRecord.import_type == 'csv',
            DataImportRecord.import_status.in_(['success', 'partial'])
        ).all()
        
        txt_records = self.db.query(DataImportRecord).filter(
            DataImportRecord.import_date == trade_date,
            DataImportRecord.import_type == 'txt',
            DataImportRecord.import_status.in_(['success', 'partial'])
        ).all()
        
        batch_records = self.db.query(DataImportRecord).filter(
            DataImportRecord.import_date == trade_date,
            DataImportRecord.import_type == 'both'
        ).all()
        
        return {
            "trade_date": trade_date.isoformat(),
            "csv_imported": len(csv_records) > 0,
            "txt_imported": len(txt_records) > 0,
            "batch_imported": len(batch_records) > 0,
            "complete": len(csv_records) > 0 and len(txt_records) > 0,
            "csv_records": len(csv_records),
            "txt_records": len(txt_records),
            "batch_records": len(batch_records),
            "ready_for_analysis": len(csv_records) > 0 and len(txt_records) > 0
        }