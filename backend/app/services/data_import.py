"""
æ•°æ®å¯¼å…¥æœåŠ¡
"""

import pandas as pd
import io
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session
from app.models import Stock, Concept, StockConcept, DailyStockData, DataImportRecord
from datetime import datetime, date


class DataImportService:
    """æ•°æ®å¯¼å…¥æœåŠ¡ç±»"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def check_existing_import(self, import_date: date, import_type: str, file_name: str) -> Optional[DataImportRecord]:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸå’Œç±»å‹çš„å¯¼å…¥è®°å½•"""
        return self.db.query(DataImportRecord).filter(
            DataImportRecord.import_date == import_date,
            DataImportRecord.import_type == import_type,
            DataImportRecord.file_name == file_name
        ).first()
    
    def create_import_record(self, import_date: date, import_type: str, file_name: str, 
                           imported_records: int = 0, skipped_records: int = 0,
                           import_status: str = 'success', error_message: str = None) -> DataImportRecord:
        """åˆ›å»ºå¯¼å…¥è®°å½•"""
        record = DataImportRecord(
            import_date=import_date,
            import_type=import_type,
            file_name=file_name,
            imported_records=imported_records,
            skipped_records=skipped_records,
            import_status=import_status,
            error_message=error_message
        )
        self.db.add(record)
        return record
    
    def update_import_record(self, record: DataImportRecord, imported_records: int = 0, 
                           skipped_records: int = 0, import_status: str = 'success', 
                           error_message: str = None):
        """æ›´æ–°å¯¼å…¥è®°å½•"""
        record.imported_records = imported_records
        record.skipped_records = skipped_records
        record.import_status = import_status
        record.error_message = error_message
        record.updated_at = datetime.now()
    
    async def import_csv_data(self, content: bytes, filename: str, allow_overwrite: bool = False) -> Dict[str, Any]:
        """
        å¯¼å…¥CSVæ ¼å¼çš„è‚¡ç¥¨æ•°æ®
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. è‹±æ–‡æ ¼å¼: stock_code,stock_name,concept,industry,date,price,turnover_rate,net_inflow
        2. ä¸­æ–‡æ ¼å¼: è‚¡ç¥¨ä»£ç ,è‚¡ç¥¨åç§°,å…¨éƒ¨é¡µæ•°,çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°,ä»·æ ¼,è¡Œä¸š,æ¦‚å¿µ,æ¢æ‰‹,å‡€æµå…¥
        """
        import_date = date.today()
        import_type = 'csv'
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¯¼å…¥è¿‡
            existing_record = self.check_existing_import(import_date, import_type, filename)
            if existing_record and not allow_overwrite:
                return {
                    "message": "ä»Šæ—¥å·²å¯¼å…¥è¯¥æ–‡ä»¶ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®allow_overwrite=True",
                    "imported_records": existing_record.imported_records,
                    "skipped_records": existing_record.skipped_records,
                    "import_date": import_date.isoformat(),
                    "already_exists": True
                }
            
            # è§£æCSVå†…å®¹
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8')
            
            # æ£€æµ‹CSVæ ¼å¼å¹¶è¿›è¡Œåˆ—åæ˜ å°„
            df = self._normalize_csv_columns(df)
            
            imported_records = 0
            skipped_records = 0
            errors = []
            
            # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['stock_code', 'stock_name', 'concept']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise Exception(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}")
            
            # å¦‚æœæ˜¯è¦†ç›–å¯¼å…¥ï¼Œå…ˆåˆ é™¤å½“å¤©çš„ç›¸å…³æ•°æ®
            if allow_overwrite and existing_record:
                # åˆ é™¤å½“å¤©å¯¼å…¥çš„è‚¡ç¥¨æ•°æ®
                deleted_count = self.db.query(DailyStockData).filter(
                    DailyStockData.trade_date == import_date
                ).delete(synchronize_session=False)
                self.db.flush()  # åˆ·æ–°ä½†ä¸æäº¤ï¼Œç¡®ä¿åˆ é™¤æ“ä½œåœ¨åç»­æ’å…¥å‰ç”Ÿæ•ˆ
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {deleted_count} æ¡å½“å¤©æ•°æ®è®°å½•")
            
            # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„è‚¡ç¥¨-æ—¥æœŸç»„åˆï¼Œé¿å…é‡å¤æ’å…¥DailyStockData
            processed_stock_dates = set()
            
            for index, row in df.iterrows():
                try:
                    # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆè¡Œ
                    if pd.isna(row.get('stock_code')) or pd.isna(row.get('stock_name')):
                        skipped_records += 1
                        continue
                    
                    # å¤„ç†è‚¡ç¥¨ä¿¡æ¯
                    stock_code = str(row['stock_code']).strip()
                    stock_name = str(row['stock_name']).strip()
                    industry = str(row.get('industry', '')).strip() if not pd.isna(row.get('industry')) else ''
                    concept_name = str(row['concept']).strip()
                    
                    # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                    if not stock_code or not stock_name or not concept_name:
                        skipped_records += 1
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºè½¬å€ºï¼ˆä»¥1å¼€å¤´çš„6ä½ä»£ç ï¼‰
                    is_convertible_bond = (
                        len(stock_code) == 6 and 
                        stock_code.startswith('1') and 
                        stock_code.isdigit()
                    )
                    
                    # è·å–æˆ–åˆ›å»ºè‚¡ç¥¨è®°å½•
                    stock = self.db.query(Stock).filter(
                        Stock.stock_code == stock_code
                    ).first()
                    
                    if not stock:
                        stock = Stock(
                            stock_code=stock_code,
                            stock_name=stock_name,
                            industry=industry if industry else None,
                            is_convertible_bond=is_convertible_bond
                        )
                        self.db.add(stock)
                        self.db.flush()  # è·å–ID
                    
                    # è·å–æˆ–åˆ›å»ºæ¦‚å¿µè®°å½•
                    concept = self.db.query(Concept).filter(
                        Concept.concept_name == concept_name
                    ).first()
                    
                    if not concept:
                        concept = Concept(concept_name=concept_name)
                        self.db.add(concept)
                        self.db.flush()  # è·å–ID
                    
                    # åˆ›å»ºè‚¡ç¥¨æ¦‚å¿µå…³è”ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    stock_concept = self.db.query(StockConcept).filter(
                        StockConcept.stock_id == stock.id,
                        StockConcept.concept_id == concept.id
                    ).first()
                    
                    if not stock_concept:
                        stock_concept = StockConcept(
                            stock_id=stock.id,
                            concept_id=concept.id
                        )
                        self.db.add(stock_concept)
                    
                    # å¤„ç†æ¯æ—¥æ•°æ®ï¼ˆç°åœ¨æ€»æ˜¯æœ‰æ—¥æœŸä¿¡æ¯ï¼‰
                    if 'date' in df.columns:
                        trade_date = pd.to_datetime(row['date']).date()
                        stock_date_key = (stock.id, trade_date)
                        
                        # åªæœ‰åœ¨æœªå¤„ç†è¿‡è¯¥è‚¡ç¥¨-æ—¥æœŸç»„åˆæ—¶æ‰å¤„ç†DailyStockData
                        if stock_date_key not in processed_stock_dates:
                            price = float(row.get('price', 0)) if pd.notna(row.get('price')) else 0
                            turnover_rate = float(row.get('turnover_rate', 0)) if pd.notna(row.get('turnover_rate')) else 0
                            net_inflow = float(row.get('net_inflow', 0)) if pd.notna(row.get('net_inflow')) else 0
                            pages_count = int(row.get('pages_count', 0)) if pd.notna(row.get('pages_count')) else 0
                            total_reads = int(row.get('total_reads', 0)) if pd.notna(row.get('total_reads')) else 0
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ—¥æœŸçš„æ•°æ®
                            existing_data = self.db.query(DailyStockData).filter(
                                DailyStockData.stock_id == stock.id,
                                DailyStockData.trade_date == trade_date
                            ).first()
                            
                            if existing_data:
                                # æ›´æ–°ç°æœ‰è®°å½•
                                existing_data.price = price
                                existing_data.turnover_rate = turnover_rate
                                existing_data.net_inflow = net_inflow
                                existing_data.pages_count = pages_count
                                existing_data.total_reads = total_reads
                            else:
                                # åˆ›å»ºæ–°è®°å½•
                                daily_data = DailyStockData(
                                    stock_id=stock.id,
                                    trade_date=trade_date,
                                    price=price,
                                    turnover_rate=turnover_rate,
                                    net_inflow=net_inflow,
                                    pages_count=pages_count,
                                    total_reads=total_reads,
                                    heat_value=0  # é»˜è®¤çƒ­åº¦å€¼ä¸º0
                                )
                                self.db.add(daily_data)
                            
                            # æ ‡è®°è¯¥è‚¡ç¥¨-æ—¥æœŸç»„åˆå·²å¤„ç†
                            processed_stock_dates.add(stock_date_key)
                    
                    imported_records += 1
                    
                except Exception as e:
                    skipped_records += 1
                    errors.append(f"ç¬¬{index+1}è¡Œ: {str(e)}")
                    continue
            
            # åˆ›å»ºæˆ–æ›´æ–°å¯¼å…¥è®°å½•
            if existing_record and allow_overwrite:
                self.update_import_record(
                    existing_record, imported_records, skipped_records, 
                    'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None  # åªä¿å­˜å‰5ä¸ªé”™è¯¯
                )
            else:
                self.create_import_record(
                    import_date, import_type, filename, imported_records, 
                    skipped_records, 'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            
            # æäº¤äº‹åŠ¡
            self.db.commit()
            
            return {
                "imported_records": imported_records,
                "skipped_records": skipped_records,
                "errors": errors,
                "import_date": import_date.isoformat(),
                "overwrite": allow_overwrite and existing_record is not None
            }
            
        except Exception as e:
            self.db.rollback()
            raise Exception(f"CSVè§£æå¤±è´¥: {str(e)}")
    
    async def import_txt_data(self, content: bytes, filename: str, allow_overwrite: bool = False) -> Dict[str, Any]:
        """
        å¯¼å…¥TXTæ ¼å¼çš„çƒ­åº¦æ•°æ®
        TXTæ ¼å¼: æ¯è¡ŒåŒ…å«è‚¡ç¥¨ä»£ç å’Œçƒ­åº¦å€¼ï¼Œç”¨åˆ¶è¡¨ç¬¦æˆ–ç©ºæ ¼åˆ†éš”
        """
        # ä»æ–‡ä»¶åå°è¯•è§£ææ—¥æœŸï¼ˆå‡è®¾æ ¼å¼å¦‚ï¼šdata_20240821.txtï¼‰
        trade_date = self._extract_date_from_filename(filename)
        if not trade_date:
            trade_date = date.today()
        
        import_type = 'txt'
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¯¼å…¥è¿‡
            existing_record = self.check_existing_import(trade_date, import_type, filename)
            if existing_record and not allow_overwrite:
                return {
                    "message": "ä»Šæ—¥å·²å¯¼å…¥è¯¥æ–‡ä»¶ï¼Œå¦‚éœ€è¦†ç›–è¯·è®¾ç½®allow_overwrite=True",
                    "imported_records": existing_record.imported_records,
                    "skipped_records": existing_record.skipped_records,
                    "import_date": trade_date.isoformat(),
                    "already_exists": True
                }
            
            # è§£æTXTå†…å®¹
            text_content = content.decode('utf-8')
            lines = text_content.strip().split('\n')
            
            imported_records = 0
            skipped_records = 0
            errors = []
            
            # å¦‚æœæ˜¯è¦†ç›–å¯¼å…¥ï¼Œå…ˆåˆ é™¤å½“å¤©çš„çƒ­åº¦æ•°æ®
            if allow_overwrite and existing_record:
                # åªåˆ é™¤çƒ­åº¦å€¼ï¼Œä¸åˆ é™¤å…¶ä»–æ•°æ®
                self.db.query(DailyStockData).filter(
                    DailyStockData.trade_date == trade_date
                ).update({"heat_value": 0}, synchronize_session=False)
            
            for line_num, line in enumerate(lines, 1):
                try:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # åˆ†å‰²æ•°æ®éƒ¨åˆ†
                    parts = line.split()
                    if len(parts) < 2:
                        skipped_records += 1
                        continue
                    
                    # å°è¯•ä¸åŒçš„æ ¼å¼
                    stock_code = None
                    heat_value = None
                    
                    # æ ¼å¼1: è‚¡ç¥¨ä»£ç  çƒ­åº¦å€¼
                    # æ ¼å¼2: æ—¥æœŸ è‚¡ç¥¨ä»£ç  çƒ­åº¦å€¼ ç­‰å…¶ä»–å­—æ®µ
                    if len(parts) == 2:
                        # æ ¼å¼1: stock_code heat_value
                        try:
                            stock_code = parts[0].strip()
                            heat_value = float(parts[1])
                        except ValueError:
                            skipped_records += 1
                            continue
                    elif len(parts) >= 3:
                        # æ ¼å¼2: date stock_code heat_value ...
                        try:
                            # å°è¯•ç¬¬äºŒä¸ªå­—æ®µä½œä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬ä¸‰ä¸ªä½œä¸ºçƒ­åº¦å€¼
                            stock_code = parts[1].strip()
                            heat_value = float(parts[2])
                        except ValueError:
                            try:
                                # å¦‚æœå¤±è´¥ï¼Œå°è¯•ç¬¬ä¸€ä¸ªå­—æ®µä½œä¸ºè‚¡ç¥¨ä»£ç ï¼Œç¬¬äºŒä¸ªä½œä¸ºçƒ­åº¦å€¼
                                stock_code = parts[0].strip()
                                heat_value = float(parts[1])
                            except ValueError:
                                skipped_records += 1
                                continue
                    else:
                        skipped_records += 1
                        continue
                    
                    # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆ6ä½æ•°å­—ï¼‰
                    if not stock_code or not stock_code.isdigit() or len(stock_code) != 6:
                        skipped_records += 1
                        errors.append(f"ç¬¬{line_num}è¡Œ: è‚¡ç¥¨ä»£ç æ ¼å¼æ— æ•ˆ {stock_code}")
                        continue
                    
                    # æŸ¥æ‰¾è‚¡ç¥¨è®°å½•
                    stock = self.db.query(Stock).filter(
                        Stock.stock_code == stock_code
                    ).first()
                    
                    if not stock:
                        errors.append(f"ç¬¬{line_num}è¡Œ: è‚¡ç¥¨ä»£ç  {stock_code} ä¸å­˜åœ¨")
                        skipped_records += 1
                        continue
                    
                    # æ›´æ–°æˆ–åˆ›å»ºæ¯æ—¥æ•°æ®
                    daily_data = self.db.query(DailyStockData).filter(
                        DailyStockData.stock_id == stock.id,
                        DailyStockData.trade_date == trade_date
                    ).first()
                    
                    if daily_data:
                        # æ›´æ–°ç°æœ‰è®°å½•çš„çƒ­åº¦å€¼
                        daily_data.heat_value = heat_value
                    else:
                        # åˆ›å»ºæ–°çš„æ¯æ—¥æ•°æ®è®°å½•
                        daily_data = DailyStockData(
                            stock_id=stock.id,
                            trade_date=trade_date,
                            heat_value=heat_value
                        )
                        self.db.add(daily_data)
                    
                    imported_records += 1
                    
                except Exception as e:
                    skipped_records += 1
                    errors.append(f"ç¬¬{line_num}è¡Œ: {str(e)}")
                    continue
            
            # åˆ›å»ºæˆ–æ›´æ–°å¯¼å…¥è®°å½•
            if existing_record and allow_overwrite:
                self.update_import_record(
                    existing_record, imported_records, skipped_records,
                    'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            else:
                self.create_import_record(
                    trade_date, import_type, filename, imported_records,
                    skipped_records, 'success' if not errors else 'partial',
                    '\n'.join(errors[:5]) if errors else None
                )
            
            # æäº¤äº‹åŠ¡
            self.db.commit()
            
            return {
                "imported_records": imported_records,
                "skipped_records": skipped_records,
                "errors": errors,
                "import_date": trade_date.isoformat(),
                "overwrite": allow_overwrite and existing_record is not None
            }
            
        except Exception as e:
            self.db.rollback()
            raise Exception(f"TXTè§£æå¤±è´¥: {str(e)}")
    
    def _extract_date_from_filename(self, filename: str) -> date:
        """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ"""
        import re
        
        # åŒ¹é… YYYYMMDD æ ¼å¼
        date_pattern = r'(\d{8})'
        match = re.search(date_pattern, filename)
        
        if match:
            date_str = match.group(1)
            try:
                return datetime.strptime(date_str, '%Y%m%d').date()
            except ValueError:
                pass
        
        return None
    
    def _normalize_csv_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–CSVåˆ—åï¼Œæ”¯æŒä¸­è‹±æ–‡æ ¼å¼è‡ªåŠ¨è½¬æ¢
        """
        # ä¸­æ–‡åˆ°è‹±æ–‡çš„åˆ—åæ˜ å°„
        chinese_to_english = {
            'è‚¡ç¥¨ä»£ç ': 'stock_code',
            'è‚¡ç¥¨åç§°': 'stock_name',
            'æ¦‚å¿µ': 'concept',
            'è¡Œä¸š': 'industry',
            'ä»·æ ¼': 'price',
            'æ¢æ‰‹': 'turnover_rate',
            'å‡€æµå…¥': 'net_inflow',
            'å…¨éƒ¨é¡µæ•°': 'pages_count',
            'çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°': 'total_reads'
        }
        
        # æ£€æµ‹æ˜¯å¦ä¸ºä¸­æ–‡æ ¼å¼
        columns = df.columns.tolist()
        is_chinese_format = any(col in chinese_to_english for col in columns)
        
        if is_chinese_format:
            # é‡å‘½ååˆ—
            df = df.rename(columns=chinese_to_english)
            
            # æ·»åŠ dateåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'date' not in df.columns:
                df['date'] = date.today().strftime('%Y-%m-%d')
            
            # å¤„ç†æ•°æ®ç±»å‹å’Œæ¸…ç†
            if 'industry' in df.columns:
                df['industry'] = df['industry'].replace('None', '').fillna('')
            
            if 'price' in df.columns:
                df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)
                
            if 'turnover_rate' in df.columns:
                df['turnover_rate'] = pd.to_numeric(df['turnover_rate'], errors='coerce').fillna(0)
                
            if 'net_inflow' in df.columns:
                df['net_inflow'] = pd.to_numeric(df['net_inflow'], errors='coerce').fillna(0)
        
        return df