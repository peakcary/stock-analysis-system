"""
ç®€åŒ–å¯¼å…¥æœåŠ¡ - ä¸“æ³¨äºå¿«é€Ÿå¯¼å…¥å¤§æ•°æ®é‡æ–‡ä»¶
"""

import pandas as pd
import io
from datetime import datetime, date
from typing import Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.models.simple_import import StockConceptData, StockTimeseriesData, ImportTask, FileType, ImportStatus


class SimpleImportService:
    """ç®€åŒ–å¯¼å…¥æœåŠ¡"""
    
    def __init__(self, db: Session):
        self.db = db
    
    async def import_csv_file(self, content: bytes, filename: str, replace_mode: str = "update") -> Dict[str, Any]:
        """
        å¯¼å…¥CSVæ–‡ä»¶ (è‚¡ç¥¨æ¦‚å¿µæ•°æ®)
        æ ¼å¼: è‚¡ç¥¨ä»£ç ,è‚¡ç¥¨åç§°,å…¨éƒ¨é¡µæ•°,çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°,ä»·æ ¼,è¡Œä¸š,æ¦‚å¿µ,æ¢æ‰‹,å‡€æµå…¥
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            replace_mode: é‡å¤å¤„ç†æ¨¡å¼
                - "update": æ›´æ–°æ¨¡å¼ï¼Œç›¸åŒå¯¼å…¥æ—¥æœŸçš„æ•°æ®ä¼šè¢«æ›´æ–°ï¼ˆé»˜è®¤ï¼‰
                - "append": è¿½åŠ æ¨¡å¼ï¼Œå…è®¸é‡å¤æ•°æ®
                - "replace": æ›¿æ¢æ¨¡å¼ï¼Œåˆ é™¤å½“å¤©æ‰€æœ‰æ•°æ®åé‡æ–°å¯¼å…¥
                - "sync": åŒæ­¥æ¨¡å¼ï¼Œä¸æ–‡ä»¶å†…å®¹å®Œå…¨åŒæ­¥ï¼Œåˆ é™¤æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„è®°å½•
        """
        import_task = ImportTask(
            file_name=filename,
            file_type='csv',  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
            file_size=len(content),
            start_time=datetime.now(),
            status='processing'  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
        )
        self.db.add(import_task)
        self.db.commit()
        self.db.refresh(import_task)
        
        try:
            # è§£æCSV
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8')
            total_rows = len(df)
            
            # æ›´æ–°æ€»è¡Œæ•°
            import_task.total_rows = total_rows
            self.db.commit()
            
            success_rows = 0
            error_rows = 0
            import_date = date.today()
            
            # æ ¹æ®æ¨¡å¼å¤„ç†é‡å¤æ•°æ®
            if replace_mode == "replace":
                # åˆ é™¤å½“å¤©çš„æ‰€æœ‰æ¦‚å¿µæ•°æ®
                deleted_count = self.db.query(StockConceptData).filter(
                    StockConceptData.import_date == import_date
                ).delete(synchronize_session=False)
                self.db.commit()
                print(f"ğŸ—‘ï¸ æ›¿æ¢æ¨¡å¼ï¼šåˆ é™¤äº† {deleted_count} æ¡å½“å¤©çš„å†å²æ•°æ®")
            elif replace_mode == "sync":
                # åŒæ­¥æ¨¡å¼ï¼šå…ˆæ ‡è®°è¦ä¿ç•™çš„æ•°æ®ï¼Œæœ€ååˆ é™¤æœªæ ‡è®°çš„æ•°æ®
                # è¿™ä¸ªé€»è¾‘ä¼šåœ¨æ•°æ®å¤„ç†å®Œæˆåæ‰§è¡Œ
                print(f"ğŸ”„ åŒæ­¥æ¨¡å¼ï¼šå°†å®Œå…¨åŒæ­¥å½“å¤©çš„æ•°æ®ï¼Œåˆ é™¤æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„è®°å½•")
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_data = []
            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000æ¡
            processed_keys = set()  # ç”¨äºSyncæ¨¡å¼è·Ÿè¸ªå¤„ç†è¿‡çš„è®°å½•
            
            for index, row in df.iterrows():
                try:
                    # æ¸…ç†æ•°æ®
                    stock_code = str(row.get('è‚¡ç¥¨ä»£ç ', '')).strip()
                    stock_name = str(row.get('è‚¡ç¥¨åç§°', '')).strip()
                    
                    if not stock_code or not stock_name or stock_code == 'nan' or stock_name == 'nan':
                        error_rows += 1
                        continue
                    
                    # å‡†å¤‡æ•°æ®
                    data_item = {
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'page_count': self._safe_int(row.get('å…¨éƒ¨é¡µæ•°', 0)),
                        'total_reads': self._safe_int(row.get('çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°', 0)),
                        'price': self._safe_float(row.get('ä»·æ ¼', 0)),
                        'industry': self._safe_str(row.get('è¡Œä¸š')),
                        'concept': self._safe_str(row.get('æ¦‚å¿µ')),
                        'turnover_rate': self._safe_float(row.get('æ¢æ‰‹', 0)),
                        'net_inflow': self._safe_float(row.get('å‡€æµå…¥', 0)),
                        'import_date': import_date,
                        'created_at': datetime.now()
                    }
                    
                    batch_data.append(data_item)
                    success_rows += 1
                    
                    # Syncæ¨¡å¼ä¸‹è®°å½•å¤„ç†è¿‡çš„key
                    if replace_mode == "sync":
                        sync_key = f"{stock_code}_{self._safe_str(row.get('æ¦‚å¿µ'))}"
                        processed_keys.add(sync_key)
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._batch_insert_concept_data(batch_data, replace_mode, import_date)
                        batch_data = []
                        
                        # æ›´æ–°è¿›åº¦
                        import_task.success_rows = success_rows
                        import_task.error_rows = error_rows
                        self.db.commit()
                        
                        print(f"ğŸ“Š å·²å¤„ç† {success_rows + error_rows}/{total_rows} è¡Œ")
                        
                except Exception as e:
                    error_rows += 1
                    print(f"è¡Œ {index + 1} å¤„ç†é”™è¯¯: {str(e)}")
            
            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_data:
                self._batch_insert_concept_data(batch_data, replace_mode, import_date)
            
            # Syncæ¨¡å¼ï¼šåˆ é™¤æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„è®°å½•
            if replace_mode == "sync":
                deleted_count = self._cleanup_sync_data(processed_keys, import_date, "concept")
                print(f"ğŸ”„ åŒæ­¥æ¨¡å¼ï¼šåˆ é™¤äº† {deleted_count} æ¡æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„è®°å½•")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            import_task.success_rows = success_rows
            import_task.error_rows = error_rows
            import_task.status = 'completed'
            import_task.end_time = datetime.now()
            self.db.commit()
            
            return {
                "task_id": import_task.id,
                "success": True,
                "total_rows": total_rows,
                "success_rows": success_rows,
                "error_rows": error_rows,
                "message": f"CSVæ–‡ä»¶å¯¼å…¥å®Œæˆï¼šæˆåŠŸ{success_rows}æ¡ï¼Œå¤±è´¥{error_rows}æ¡"
            }
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
            import_task.status = 'failed'
            import_task.error_message = str(e)
            import_task.end_time = datetime.now()
            self.db.commit()
            
            raise Exception(f"CSVå¯¼å…¥å¤±è´¥: {str(e)}")
    
    async def import_txt_file(self, content: bytes, filename: str, replace_mode: str = "update") -> Dict[str, Any]:
        """
        å¯¼å…¥TXTæ–‡ä»¶ (è‚¡ç¥¨æ—¶é—´åºåˆ—æ•°æ®)
        æ ¼å¼: è‚¡ç¥¨ä»£ç \tæ—¥æœŸ\tæ•°å€¼
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            replace_mode: é‡å¤å¤„ç†æ¨¡å¼
                - "update": æ›´æ–°æ¨¡å¼ï¼Œç›¸åŒè‚¡ç¥¨ä»£ç +æ—¥æœŸçš„æ•°æ®ä¼šè¢«æ›´æ–°ï¼ˆé»˜è®¤ï¼‰
                - "append": è¿½åŠ æ¨¡å¼ï¼Œå…è®¸é‡å¤æ•°æ®
                - "replace": æ›¿æ¢æ¨¡å¼ï¼Œåˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ•°æ®åé‡æ–°å¯¼å…¥
        """
        import_task = ImportTask(
            file_name=filename,
            file_type='txt',  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
            file_size=len(content),
            start_time=datetime.now(),
            status='processing'  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
        )
        self.db.add(import_task)
        self.db.commit()
        self.db.refresh(import_task)
        
        try:
            # è§£æTXTå†…å®¹
            text_content = content.decode('utf-8')
            lines = text_content.strip().split('\n')
            total_rows = len(lines)
            
            # æ›´æ–°æ€»è¡Œæ•°
            import_task.total_rows = total_rows
            self.db.commit()
            
            success_rows = 0
            error_rows = 0
            import_date = date.today()
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_data = []
            batch_size = 5000  # TXTæ–‡ä»¶æ•°æ®é‡å¤§ï¼Œå¢å¤§æ‰¹æ¬¡
            
            for line_num, line in enumerate(lines, 1):
                try:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # åˆ†å‰²æ•°æ®ï¼šè‚¡ç¥¨ä»£ç \tæ—¥æœŸ\tæ•°å€¼
                    parts = line.split('\t')
                    if len(parts) < 2:
                        error_rows += 1
                        if error_rows <= 5:
                            print(f"âŒ è¡Œ {line_num} å­—æ®µæ•°é‡ä¸è¶³: æœ€å°‘éœ€è¦2ä¸ªå­—æ®µï¼Œå®é™…{len(parts)}ä¸ª - '{line}'")
                        continue
                    elif len(parts) == 2:
                        # å¦‚æœåªæœ‰2ä¸ªå­—æ®µï¼ˆè‚¡ç¥¨ä»£ç å’Œæ—¥æœŸï¼‰ï¼Œæ•°å€¼é»˜è®¤ä¸º0
                        parts.append('0')
                        if success_rows <= 5:  # è®°å½•å‰5ä¸ªè‡ªåŠ¨è¡¥0çš„æƒ…å†µ
                            print(f"â„¹ï¸ è¡Œ {line_num} ç¼ºå°‘æ•°å€¼å­—æ®µï¼Œè‡ªåŠ¨è¡¥0 - '{line}'")
                    elif len(parts) > 3:
                        error_rows += 1
                        if error_rows <= 5:
                            print(f"âŒ è¡Œ {line_num} å­—æ®µæ•°é‡è¿‡å¤š: æœŸæœ›3ä¸ªå­—æ®µï¼Œå®é™…{len(parts)}ä¸ª - '{line}'")
                        continue
                    
                    stock_code = parts[0].strip()
                    trade_date_str = parts[1].strip()
                    value_str = parts[2].strip()
                    
                    # éªŒè¯å’Œè½¬æ¢æ•°æ®
                    if not stock_code:
                        error_rows += 1
                        if error_rows <= 5:
                            print(f"âŒ è¡Œ {line_num} è‚¡ç¥¨ä»£ç ä¸ºç©º - '{line}'")
                        continue
                    
                    # å»é™¤è‚¡ç¥¨ä»£ç å‰ç¼€ï¼ˆå¦‚SHã€SZï¼‰
                    if stock_code.startswith(('SH', 'SZ')):
                        stock_code = stock_code[2:]
                    
                    try:
                        trade_date = datetime.strptime(trade_date_str, '%Y-%m-%d').date()
                        value = float(value_str)
                    except ValueError as ve:
                        error_rows += 1
                        if error_rows <= 5:
                            print(f"âŒ è¡Œ {line_num} æ•°æ®æ ¼å¼é”™è¯¯: {str(ve)} - '{line}'")
                        continue
                    
                    # å‡†å¤‡æ•°æ®
                    data_item = {
                        'stock_code': stock_code,
                        'trade_date': trade_date,
                        'value': value,
                        'import_date': import_date,
                        'created_at': datetime.now()
                    }
                    
                    batch_data.append(data_item)
                    success_rows += 1
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._batch_insert_timeseries_data(batch_data)
                        batch_data = []
                        
                        # æ›´æ–°è¿›åº¦
                        import_task.success_rows = success_rows
                        import_task.error_rows = error_rows
                        self.db.commit()
                        
                        if success_rows % 50000 == 0:  # æ¯5ä¸‡æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                            print(f"ğŸ“Š å·²å¤„ç† {success_rows + error_rows}/{total_rows} è¡Œ")
                        
                except Exception as e:
                    error_rows += 1
                    if error_rows <= 10:  # è®°å½•å‰10ä¸ªå¤„ç†å¼‚å¸¸
                        print(f"âŒ è¡Œ {line_num} å¤„ç†å¼‚å¸¸: {str(e)} - '{line[:100]}...'")
                    elif error_rows % 10000 == 0:  # æ¯1ä¸‡ä¸ªé”™è¯¯è®°å½•ä¸€æ¬¡
                        print(f"âŒ å·²å¤„ç† {error_rows} ä¸ªé”™è¯¯è¡Œ")
            
            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_data:
                self._batch_insert_timeseries_data(batch_data)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            import_task.success_rows = success_rows
            import_task.error_rows = error_rows
            import_task.status = 'completed'
            import_task.end_time = datetime.now()
            self.db.commit()
            
            return {
                "task_id": import_task.id,
                "success": True,
                "total_rows": total_rows,
                "success_rows": success_rows,
                "error_rows": error_rows,
                "message": f"TXTæ–‡ä»¶å¯¼å…¥å®Œæˆï¼šæˆåŠŸ{success_rows}æ¡ï¼Œå¤±è´¥{error_rows}æ¡"
            }
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
            import_task.status = 'failed'
            import_task.error_message = str(e)
            import_task.end_time = datetime.now()
            self.db.commit()
            
            raise Exception(f"TXTå¯¼å…¥å¤±è´¥: {str(e)}")
    
    def _batch_insert_concept_data(self, batch_data, replace_mode: str = "update", import_date=None):
        """
        æ‰¹é‡æ’å…¥æ¦‚å¿µæ•°æ®
        
        Args:
            batch_data: è¦æ’å…¥çš„æ•°æ®åˆ—è¡¨
            replace_mode: é‡å¤å¤„ç†æ¨¡å¼ (update/append/replace)
            import_date: å¯¼å…¥æ—¥æœŸï¼Œç”¨äºæ›¿æ¢æ¨¡å¼
        """
        try:
            if replace_mode == "append":
                # è¿½åŠ æ¨¡å¼ï¼šç›´æ¥æ’å…¥ï¼Œå…è®¸é‡å¤
                self.db.bulk_insert_mappings(StockConceptData, batch_data)
                self.db.commit()
            else:
                # æ›´æ–°æ¨¡å¼ï¼šé€æ¡æ£€æŸ¥å¹¶å¤„ç†é‡å¤
                self._upsert_concept_data(batch_data, import_date)
        except Exception as e:
            self.db.rollback()
            # å¦‚æœæ‰¹é‡æ“ä½œå¤±è´¥ï¼Œé™çº§åˆ°é€æ¡å¤„ç†
            self._fallback_insert_concept_data(batch_data, replace_mode, import_date)
    
    def _upsert_concept_data(self, batch_data, import_date):
        """æ›´æ–°æ’å…¥æ¦‚å¿µæ•°æ®"""
        for data in batch_data:
            try:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒçš„è®°å½•ï¼ˆåŸºäºè‚¡ç¥¨ä»£ç +æ¦‚å¿µ+å¯¼å…¥æ—¥æœŸï¼‰
                existing = self.db.query(StockConceptData).filter(
                    StockConceptData.stock_code == data['stock_code'],
                    StockConceptData.concept == data['concept'],
                    StockConceptData.import_date == import_date
                ).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for key, value in data.items():
                        if key != 'created_at':  # ä¸æ›´æ–°åˆ›å»ºæ—¶é—´
                            setattr(existing, key, value)
                else:
                    # æ’å…¥æ–°è®°å½•
                    record = StockConceptData(**data)
                    self.db.add(record)
                    
            except Exception:
                self.db.rollback()
                continue
        
        self.db.commit()
    
    def _fallback_insert_concept_data(self, batch_data, replace_mode, import_date):
        """æ¦‚å¿µæ•°æ®æ’å…¥å¤±è´¥åçš„é™çº§å¤„ç†"""
        for data in batch_data:
            try:
                if replace_mode == "append":
                    record = StockConceptData(**data)
                    self.db.add(record)
                else:
                    # æ›´æ–°æ¨¡å¼çš„é€æ¡å¤„ç†
                    existing = self.db.query(StockConceptData).filter(
                        StockConceptData.stock_code == data['stock_code'],
                        StockConceptData.concept == data['concept'],
                        StockConceptData.import_date == import_date
                    ).first()
                    
                    if existing:
                        for key, value in data.items():
                            if key != 'created_at':
                                setattr(existing, key, value)
                    else:
                        record = StockConceptData(**data)
                        self.db.add(record)
                
                self.db.commit()
            except Exception:
                self.db.rollback()
                continue
    
    def _batch_insert_timeseries_data(self, batch_data):
        """æ‰¹é‡æ’å…¥æ—¶é—´åºåˆ—æ•°æ®"""
        try:
            # ä½¿ç”¨ON DUPLICATE KEY UPDATEå¤„ç†é‡å¤æ•°æ®
            self.db.bulk_insert_mappings(StockTimeseriesData, batch_data)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥
            for data in batch_data:
                try:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    existing = self.db.query(StockTimeseriesData).filter(
                        StockTimeseriesData.stock_code == data['stock_code'],
                        StockTimeseriesData.trade_date == data['trade_date']
                    ).first()
                    
                    if existing:
                        # æ›´æ–°ç°æœ‰è®°å½•
                        existing.value = data['value']
                        existing.import_date = data['import_date']
                    else:
                        # æ’å…¥æ–°è®°å½•
                        record = StockTimeseriesData(**data)
                        self.db.add(record)
                    
                    self.db.commit()
                except Exception:
                    self.db.rollback()
                    continue
    
    def get_import_task_status(self, task_id: int) -> Dict[str, Any]:
        """è·å–å¯¼å…¥ä»»åŠ¡çŠ¶æ€"""
        task = self.db.query(ImportTask).filter(ImportTask.id == task_id).first()
        if not task:
            return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
        
        duration = None
        if task.start_time and task.end_time:
            duration = (task.end_time - task.start_time).total_seconds()
        
        return {
            "task_id": task.id,
            "file_name": task.file_name,
            "file_type": task.file_type,
            "file_size": task.file_size,
            "total_rows": task.total_rows,
            "success_rows": task.success_rows,
            "error_rows": task.error_rows,
            "status": task.status,
            "error_message": task.error_message,
            "start_time": task.start_time.isoformat() if task.start_time else None,
            "end_time": task.end_time.isoformat() if task.end_time else None,
            "duration_seconds": duration,
            "progress_percent": round((task.success_rows + task.error_rows) / task.total_rows * 100, 2) if task.total_rows > 0 else 0
        }
    
    @staticmethod
    def _safe_int(value) -> int:
        """å®‰å…¨è½¬æ¢æ•´æ•°"""
        try:
            if pd.isna(value) or value == 'None':
                return 0
            return int(float(value))
        except (ValueError, TypeError):
            return 0
    
    @staticmethod
    def _safe_float(value) -> float:
        """å®‰å…¨è½¬æ¢æµ®ç‚¹æ•°"""
        try:
            if pd.isna(value) or value == 'None':
                return 0.0
            return float(value)
        except (ValueError, TypeError):
            return 0.0
    
    @staticmethod
    def _safe_str(value) -> str:
        """å®‰å…¨è½¬æ¢å­—ç¬¦ä¸²"""
        try:
            if pd.isna(value) or value == 'None':
                return ''
            return str(value).strip()
        except (ValueError, TypeError):
            return ''
    
    def _cleanup_sync_data(self, processed_keys: set, import_date, data_type: str) -> int:
        """
        æ¸…ç†åŒæ­¥æ•°æ®ï¼šåˆ é™¤æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„è®°å½•
        
        Args:
            processed_keys: æ–‡ä»¶ä¸­å­˜åœ¨çš„è®°å½•keyé›†åˆ
            import_date: å¯¼å…¥æ—¥æœŸ  
            data_type: æ•°æ®ç±»å‹ ("concept" æˆ– "timeseries")
        
        Returns:
            åˆ é™¤çš„è®°å½•æ•°é‡
        """
        try:
            if data_type == "concept":
                # è·å–å½“å¤©æ•°æ®åº“ä¸­çš„æ‰€æœ‰è®°å½•
                existing_records = self.db.query(StockConceptData).filter(
                    StockConceptData.import_date == import_date
                ).all()
                
                # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•
                to_delete_ids = []
                for record in existing_records:
                    record_key = f"{record.stock_code}_{record.concept or ''}"
                    if record_key not in processed_keys:
                        to_delete_ids.append(record.id)
                
                # æ‰¹é‡åˆ é™¤
                if to_delete_ids:
                    deleted_count = self.db.query(StockConceptData).filter(
                        StockConceptData.id.in_(to_delete_ids)
                    ).delete(synchronize_session=False)
                    self.db.commit()
                    return deleted_count
                    
            elif data_type == "timeseries":
                # TXTæ—¶é—´åºåˆ—æ•°æ®çš„åŒæ­¥å¤„ç†
                # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“çš„äº¤æ˜“æ—¥æœŸæ¥å¤„ç†
                pass
                
        except Exception as e:
            self.db.rollback()
            print(f"âŒ åŒæ­¥æ¸…ç†å¤±è´¥: {str(e)}")
            
        return 0