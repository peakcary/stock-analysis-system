"""
ç®€åŒ–å¯¼å…¥æœåŠ¡ - ä¸“æ³¨äºå¿«é€Ÿå¯¼å…¥å¤§æ•°æ®é‡æ–‡ä»¶
"""

import pandas as pd
import io
from datetime import datetime, date
from typing import Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import text
from app.models.simple_import import StockConceptData, StockTimeseriesData, ImportTask, FileType, ImportStatus


class SimpleImportService:
    """ç®€åŒ–å¯¼å…¥æœåŠ¡"""
    
    def __init__(self, db: Session):
        self.db = db
    
    async def import_csv_file(self, content: bytes, filename: str) -> Dict[str, Any]:
        """
        å¯¼å…¥CSVæ–‡ä»¶ (è‚¡ç¥¨æ¦‚å¿µæ•°æ®)
        æ ¼å¼: è‚¡ç¥¨ä»£ç ,è‚¡ç¥¨åç§°,å…¨éƒ¨é¡µæ•°,çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°,ä»·æ ¼,è¡Œä¸š,æ¦‚å¿µ,æ¢æ‰‹,å‡€æµå…¥
        """
        import_task = ImportTask(
            file_name=filename,
            file_type='csv',  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
            file_size=len(content),
            start_time=datetime.now(),
            status='processing'  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
        )
        self.db.add(import_task)
        self.db.commit()
        self.db.refresh(import_task)
        
        try:
            # è§£æCSV
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8')
            total_rows = len(df)
            
            # æ›´æ–°æ€»è¡Œæ•°
            import_task.total_rows = total_rows
            self.db.commit()
            
            success_rows = 0
            error_rows = 0
            import_date = date.today()
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_data = []
            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000æ¡
            
            for index, row in df.iterrows():
                try:
                    # æ¸…ç†æ•°æ®
                    stock_code = str(row.get('è‚¡ç¥¨ä»£ç ', '')).strip()
                    stock_name = str(row.get('è‚¡ç¥¨åç§°', '')).strip()
                    
                    if not stock_code or not stock_name or stock_code == 'nan' or stock_name == 'nan':
                        error_rows += 1
                        continue
                    
                    # å‡†å¤‡æ•°æ®
                    data_item = {
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'page_count': self._safe_int(row.get('å…¨éƒ¨é¡µæ•°', 0)),
                        'total_reads': self._safe_int(row.get('çƒ­å¸–é¦–é¡µé¡µé˜…è¯»æ€»æ•°', 0)),
                        'price': self._safe_float(row.get('ä»·æ ¼', 0)),
                        'industry': self._safe_str(row.get('è¡Œä¸š')),
                        'concept': self._safe_str(row.get('æ¦‚å¿µ')),
                        'turnover_rate': self._safe_float(row.get('æ¢æ‰‹', 0)),
                        'net_inflow': self._safe_float(row.get('å‡€æµå…¥', 0)),
                        'import_date': import_date,
                        'created_at': datetime.now()
                    }
                    
                    batch_data.append(data_item)
                    success_rows += 1
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._batch_insert_concept_data(batch_data)
                        batch_data = []
                        
                        # æ›´æ–°è¿›åº¦
                        import_task.success_rows = success_rows
                        import_task.error_rows = error_rows
                        self.db.commit()
                        
                        print(f"ğŸ“Š å·²å¤„ç† {success_rows + error_rows}/{total_rows} è¡Œ")
                        
                except Exception as e:
                    error_rows += 1
                    print(f"è¡Œ {index + 1} å¤„ç†é”™è¯¯: {str(e)}")
            
            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_data:
                self._batch_insert_concept_data(batch_data)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            import_task.success_rows = success_rows
            import_task.error_rows = error_rows
            import_task.status = 'completed'
            import_task.end_time = datetime.now()
            self.db.commit()
            
            return {
                "task_id": import_task.id,
                "success": True,
                "total_rows": total_rows,
                "success_rows": success_rows,
                "error_rows": error_rows,
                "message": f"CSVæ–‡ä»¶å¯¼å…¥å®Œæˆï¼šæˆåŠŸ{success_rows}æ¡ï¼Œå¤±è´¥{error_rows}æ¡"
            }
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
            import_task.status = 'failed'
            import_task.error_message = str(e)
            import_task.end_time = datetime.now()
            self.db.commit()
            
            raise Exception(f"CSVå¯¼å…¥å¤±è´¥: {str(e)}")
    
    async def import_txt_file(self, content: bytes, filename: str) -> Dict[str, Any]:
        """
        å¯¼å…¥TXTæ–‡ä»¶ (è‚¡ç¥¨æ—¶é—´åºåˆ—æ•°æ®)
        æ ¼å¼: è‚¡ç¥¨ä»£ç \tæ—¥æœŸ\tæ•°å€¼
        """
        import_task = ImportTask(
            file_name=filename,
            file_type='txt',  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
            file_size=len(content),
            start_time=datetime.now(),
            status='processing'  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
        )
        self.db.add(import_task)
        self.db.commit()
        self.db.refresh(import_task)
        
        try:
            # è§£æTXTå†…å®¹
            text_content = content.decode('utf-8')
            lines = text_content.strip().split('\n')
            total_rows = len(lines)
            
            # æ›´æ–°æ€»è¡Œæ•°
            import_task.total_rows = total_rows
            self.db.commit()
            
            success_rows = 0
            error_rows = 0
            import_date = date.today()
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_data = []
            batch_size = 5000  # TXTæ–‡ä»¶æ•°æ®é‡å¤§ï¼Œå¢å¤§æ‰¹æ¬¡
            
            for line_num, line in enumerate(lines, 1):
                try:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # åˆ†å‰²æ•°æ®ï¼šè‚¡ç¥¨ä»£ç \tæ—¥æœŸ\tæ•°å€¼
                    parts = line.split('\t')
                    if len(parts) != 3:
                        error_rows += 1
                        continue
                    
                    stock_code = parts[0].strip()
                    trade_date_str = parts[1].strip()
                    value_str = parts[2].strip()
                    
                    # éªŒè¯å’Œè½¬æ¢æ•°æ®
                    if not stock_code:
                        error_rows += 1
                        continue
                    
                    # å»é™¤è‚¡ç¥¨ä»£ç å‰ç¼€ï¼ˆå¦‚SHã€SZï¼‰
                    if stock_code.startswith(('SH', 'SZ')):
                        stock_code = stock_code[2:]
                    
                    try:
                        trade_date = datetime.strptime(trade_date_str, '%Y-%m-%d').date()
                        value = float(value_str)
                    except ValueError:
                        error_rows += 1
                        continue
                    
                    # å‡†å¤‡æ•°æ®
                    data_item = {
                        'stock_code': stock_code,
                        'trade_date': trade_date,
                        'value': value,
                        'import_date': import_date,
                        'created_at': datetime.now()
                    }
                    
                    batch_data.append(data_item)
                    success_rows += 1
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_data) >= batch_size:
                        self._batch_insert_timeseries_data(batch_data)
                        batch_data = []
                        
                        # æ›´æ–°è¿›åº¦
                        import_task.success_rows = success_rows
                        import_task.error_rows = error_rows
                        self.db.commit()
                        
                        if success_rows % 50000 == 0:  # æ¯5ä¸‡æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                            print(f"ğŸ“Š å·²å¤„ç† {success_rows + error_rows}/{total_rows} è¡Œ")
                        
                except Exception as e:
                    error_rows += 1
                    if error_rows % 10000 == 0:  # é¿å…æ—¥å¿—è¿‡å¤š
                        print(f"è¡Œ {line_num} å¤„ç†é”™è¯¯: {str(e)}")
            
            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_data:
                self._batch_insert_timeseries_data(batch_data)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            import_task.success_rows = success_rows
            import_task.error_rows = error_rows
            import_task.status = 'completed'
            import_task.end_time = datetime.now()
            self.db.commit()
            
            return {
                "task_id": import_task.id,
                "success": True,
                "total_rows": total_rows,
                "success_rows": success_rows,
                "error_rows": error_rows,
                "message": f"TXTæ–‡ä»¶å¯¼å…¥å®Œæˆï¼šæˆåŠŸ{success_rows}æ¡ï¼Œå¤±è´¥{error_rows}æ¡"
            }
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
            import_task.status = 'failed'
            import_task.error_message = str(e)
            import_task.end_time = datetime.now()
            self.db.commit()
            
            raise Exception(f"TXTå¯¼å…¥å¤±è´¥: {str(e)}")
    
    def _batch_insert_concept_data(self, batch_data):
        """æ‰¹é‡æ’å…¥æ¦‚å¿µæ•°æ®"""
        try:
            self.db.bulk_insert_mappings(StockConceptData, batch_data)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥
            for data in batch_data:
                try:
                    record = StockConceptData(**data)
                    self.db.add(record)
                    self.db.commit()
                except Exception:
                    self.db.rollback()
                    continue
    
    def _batch_insert_timeseries_data(self, batch_data):
        """æ‰¹é‡æ’å…¥æ—¶é—´åºåˆ—æ•°æ®"""
        try:
            # ä½¿ç”¨ON DUPLICATE KEY UPDATEå¤„ç†é‡å¤æ•°æ®
            self.db.bulk_insert_mappings(StockTimeseriesData, batch_data)
            self.db.commit()
        except Exception as e:
            self.db.rollback()
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥
            for data in batch_data:
                try:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    existing = self.db.query(StockTimeseriesData).filter(
                        StockTimeseriesData.stock_code == data['stock_code'],
                        StockTimeseriesData.trade_date == data['trade_date']
                    ).first()
                    
                    if existing:
                        # æ›´æ–°ç°æœ‰è®°å½•
                        existing.value = data['value']
                        existing.import_date = data['import_date']
                    else:
                        # æ’å…¥æ–°è®°å½•
                        record = StockTimeseriesData(**data)
                        self.db.add(record)
                    
                    self.db.commit()
                except Exception:
                    self.db.rollback()
                    continue
    
    def get_import_task_status(self, task_id: int) -> Dict[str, Any]:
        """è·å–å¯¼å…¥ä»»åŠ¡çŠ¶æ€"""
        task = self.db.query(ImportTask).filter(ImportTask.id == task_id).first()
        if not task:
            return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
        
        duration = None
        if task.start_time and task.end_time:
            duration = (task.end_time - task.start_time).total_seconds()
        
        return {
            "task_id": task.id,
            "file_name": task.file_name,
            "file_type": task.file_type,
            "file_size": task.file_size,
            "total_rows": task.total_rows,
            "success_rows": task.success_rows,
            "error_rows": task.error_rows,
            "status": task.status,
            "error_message": task.error_message,
            "start_time": task.start_time.isoformat() if task.start_time else None,
            "end_time": task.end_time.isoformat() if task.end_time else None,
            "duration_seconds": duration,
            "progress_percent": round((task.success_rows + task.error_rows) / task.total_rows * 100, 2) if task.total_rows > 0 else 0
        }
    
    @staticmethod
    def _safe_int(value) -> int:
        """å®‰å…¨è½¬æ¢æ•´æ•°"""
        try:
            if pd.isna(value) or value == 'None':
                return 0
            return int(float(value))
        except (ValueError, TypeError):
            return 0
    
    @staticmethod
    def _safe_float(value) -> float:
        """å®‰å…¨è½¬æ¢æµ®ç‚¹æ•°"""
        try:
            if pd.isna(value) or value == 'None':
                return 0.0
            return float(value)
        except (ValueError, TypeError):
            return 0.0
    
    @staticmethod
    def _safe_str(value) -> str:
        """å®‰å…¨è½¬æ¢å­—ç¬¦ä¸²"""
        try:
            if pd.isna(value) or value == 'None':
                return ''
            return str(value).strip()
        except (ValueError, TypeError):
            return ''